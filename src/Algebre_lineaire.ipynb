{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notions d'algèbre linéaire\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Ce document est un traduction \"libre\" et approximative du cours de Zico Kolter : [Linear Algebra Review and Reference](https://see.stanford.edu/materials/aimlcs229/cs229-linalg.pdf), aggrémenté de quelques exemples Python. Les lecteurs sont fortement encouragés à se référer au document d'origine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Concepts de base et notation\n",
    "\n",
    "L'algèbre linéaire fournit les outils permettant de représenter et manipuler simplement des ensembles d'équations linaires.\n",
    "\n",
    "\\begin{align}\n",
    "     4 x_1 - 5 x_2 & = -13 \\\\\n",
    "    -2 x_1 + 3 x_2 & =  9  \n",
    "\\end{align}\n",
    "\n",
    "Il est possible à partir de ces deux équiations à deux variables de trouver une unique solution pour $x_1$ et $x_2$ (à moins que les équations soient en quelque sorte dégénérées, par exemple si la seconde équation est un multiple de la première). Dans notre exemple il n'y a une solution unique. En notation matricielle, le système précédent peut être écrit de manière plus compacte :\n",
    "\n",
    "\\begin{equation*}\n",
    "Ax=b \\\\\n",
    "\\text{ avec } A = \\begin{bmatrix}\n",
    "    4  & -5 \\\\\n",
    "    -2 & 3\n",
    "\\end{bmatrix} \n",
    "\\text{ , }\n",
    "b = \\begin{bmatrix}\n",
    "-13 \\\\\n",
    "9\n",
    "\\end{bmatrix} \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple de définition numpy\n",
    "\n",
    "L'exemple précédent peut s'écrire de la manière suivante avec numpy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = \n",
      " [[ 4 -5]\n",
      " [-2  3]]\n",
      "b = \n",
      " [[-13]\n",
      " [  9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.matrix([[4,-5], [-2, 3]])\n",
    "b = np.matrix([-13,9]).transpose()\n",
    "print(\"A = \\n\",A)\n",
    "print(\"b = \\n\",b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Notations de base\n",
    "\n",
    "Nous utilisons les notations suivantes:\n",
    "\n",
    "- $A \\in \\mathbb{R}^{m \\times n}$ définit une matrice ayant $m$ lignes et $n$ colonnes dans laquelle les élements sont des nombres réels.\n",
    "\n",
    "- $x \\in \\mathbb{R}^n$ définit un vecteur à $n$ éléments. Généralement un vecteur $x$ est un **vecteur en colonne**, c'est à dire une matrice ayant une ligne et $n$ colonnes (soit une matrice $\\mathbb{R}^{1 \\times n}$). Si l'on veut décrire un **vecteur ligne** ayant 1 ligne et $n$ colonnes, on écrit $x^T$ ($x^T$ veut dire la transposée de $x$, que nous allons décrire bientôt). \n",
    "\n",
    "- Le $i^{eme}$ élément d'un vecteur $x$ se note $x_i$ :\n",
    "\n",
    "\\begin{equation*}\n",
    "x = \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "- La notation $a_ij$ (ou $A_{ij}$, $A_{i,j}$, etc) définit l'élément de $A$ présent à la $i^{eme}$ ligne et à la $j^{ieme}$ colonne:\n",
    "\n",
    "\\begin{equation*}\n",
    "A = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\dots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\dots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\dots & a_{mn}\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "- La $j^{eme}$ colonne de A se note $a_j$ ou $A_{:,j}$ :\n",
    "\n",
    "\\begin{equation*}\n",
    "A = \\begin{bmatrix}\n",
    "\\rvert & \\rvert\t&       & \\rvert \\\\\n",
    "a_1 & a_2 & \\dots & a_n \\\\\n",
    "\\rvert & \\rvert\t&       & \\rvert \n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "- La $i^{eme}$ ligne de A se note $a_i^T$ ou $A_{i,:}$ :\n",
    "\n",
    "\\begin{equation*}\n",
    "A = \\begin{bmatrix}\n",
    "- & a_1^T & - \\\\\n",
    "- & a_2^T & - \\\\\n",
    "  & \\vdots & \\\\\n",
    "- & a_m^T & - \n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulation avec numpy \n",
    "\n",
    "> Dans numpy la première ligne et la premiere colonne commencent à 0 et non à 1 comme dans les conventions mathématiques classiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La valeur de l'element a22 est :\n",
      " 3\n",
      "Le vecteur de la deuxieme colonne a2 est :\n",
      " [[-5]\n",
      " [ 3]]\n",
      "Le vecteur de la deuxieme ligne aT2 est :\n",
      " [[-2  3]]\n"
     ]
    }
   ],
   "source": [
    "a22 = A[1,1]\n",
    "print(\"La valeur de l'element a22 est :\\n\",a22)\n",
    "\n",
    "a2 = A[:,1]\n",
    "print(\"Le vecteur de la deuxieme colonne a2 est :\\n\",a2)\n",
    "\n",
    "aT2 = A[1,:]\n",
    "print(\"Le vecteur de la deuxieme ligne aT2 est :\\n\",aT2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplication matricielle\n",
    "\n",
    "Le produit de deux matrices $A \\in \\mathbb{R}^{m \\times n}$ et  $B \\in \\mathbb{R}^{n \\times p}$ donne la matrice:\n",
    "\n",
    "\\begin{equation*}\n",
    "C = AB \\in \\mathbb{R}^{m \\times p}, \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "où\n",
    "\n",
    "\\begin{equation*}\n",
    "C_{ij} = \\sum_{k = 1}^n A_{ik}B_{kj}\n",
    "\\end{equation*}\n",
    "\n",
    "Notez que pour que le produit matriciel existe le nombre de colonnes de $A$ doit être égal au nombre de lignes de $B$. Etudions maintenant quelques cas particuliers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplication de deux vecteurs entre eux\n",
    "\n",
    "#### Produit scalaire \n",
    "\n",
    "Etant donnés deux vecteurs $x,y \\in \\mathbb{R}^{n}$, la quantité $x^Ty$ est un nombre réel appelé le produit scalaire des deux vecteurs. Cette opération est aussi appelée **dot product** ou **inner product** en anglais.\n",
    "\n",
    "\\begin{equation*}\n",
    "x^Ty \\in \\mathbb{R} = \\sum_{i = 1}^n x_{i}y_{i}\n",
    "\\end{equation*}\n",
    "\n",
    "Notez que l'égalité suivante est toujours vérifiée : $x^Ty = y^Tx$\n",
    "On note aussi le produit scalaire de $x$ et $y$: $x \\centerdot y$.\n",
    "\n",
    "#### Produit extérieur\n",
    "\n",
    "Etant donnés deux vecteurs $x \\in \\mathbb{R}^{m}$ et $y \\in \\mathbb{R}^{n}$ (ils n'ont pas à être de la même dimension), $xy^T$ est appelé **le produit extérieur** des deux vecteurs (on le note aussi $x \\otimes y$). Le résultat est une matrice :\n",
    "\n",
    "\\begin{equation*}\n",
    "xy^T \\in \\mathbb{R}^{m \\times n} = \\begin{bmatrix}\n",
    "x_1y_1 & x_1y_2 & \\dots & x_1y_n \\\\\n",
    "x_2y_1 & x_2y_2 & \\dots & x_2y_n \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_my_1 & x_my_2 & \\dots & x_my_n\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le produit scalaire de x et y est  56\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.matrix([2,3,4])\n",
    "y = np.matrix([5,6,7]).transpose()\n",
    "s = np.dot(x,y)\n",
    "print (\"Le produit scalaire de x et y est \",s[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplication d'une matrice et d'un vecteur\n",
    "\n",
    "#### Matrice multiplié par un vecteur\n",
    "\n",
    "Etant donné une matrice $A \\in \\mathbb{R}^{m \\times n}$ et un vecteur $x \\in \\mathbb{R}^n$, leur produit est un vecteur $y = Ax \\in \\mathbb{R}^m$.\n",
    "\n",
    "Il y a deux manière d'appréhender la multiplication d'une matrice et d'un vecteur, nous allons étudier les deux:\n",
    "\n",
    "- Si nous écrivons $A$ selon ses lignes, alors nous pouvous exprimer $Ax$ comme:\n",
    "\n",
    "\\begin{equation*}\n",
    "y = \\begin{bmatrix}\n",
    "- & a_1^T & - \\\\\n",
    "- & a_2^T & - \\\\\n",
    "  & \\vdots & \\\\\n",
    "- & a_m^T & - \n",
    "\\end{bmatrix} x = \\begin{bmatrix}\n",
    "a_1^Tx \\\\\n",
    "a_2^Tx \\\\\n",
    "\\vdots \\\\\n",
    "a_m^Tx\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Autrement dit, le $i^{eme}$ élément de $y$ est égal au produit scalaire de la $i^{eme}$ ligne de $A$ et de $x$, $y_i = a_i^Tx = a_i \\centerdot x$.\n",
    "\n",
    "- Nous pouvons aussi écrire $A$ sous forme de colonnes. Dans ce cas nous voyons que:\n",
    "\n",
    "\\begin{equation*}\n",
    "y = \\begin{bmatrix}\n",
    "\\rvert & \\rvert\t&       & \\rvert \\\\\n",
    "a_1 & a_2 & \\dots & a_n \\\\\n",
    "\\rvert & \\rvert\t&       & \\rvert \n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{bmatrix} =  \\begin{bmatrix} a_1 \\end{bmatrix} x_1 + \\begin{bmatrix} a_2 \\end{bmatrix} x_2 +\n",
    "\\dots + \\begin{bmatrix} a_n \\end{bmatrix} x_n\n",
    "\\end{equation*}\n",
    "\n",
    "En d'autres termes, $y$ est une **combinaison linéaire** des colonnes de $A$, dans laquelle les coefficients de la combinaison linéaire sont données par les éléments de $x$.\n",
    "\n",
    "#### Vecteur multiplié par une matrice\n",
    "\n",
    "Jusqu'alors nous avons mutliplié à droite par un vecteur colonne, mais il est aussi possible de multiplier à gauche par un vecteur ligne. Cette opération s'écrit : $y^T = x^TA$ avec $A \\in \\mathbb{R}^{m \\times n }$, $x \\in \\mathbb{R}^m$ et $y \\in \\mathbb{R}^n$. Comme auparavant, on peut exprimer $y^T$ de deux façons:\n",
    "\n",
    "- Exprimons $A$ selon ses colonnes:\n",
    "\n",
    "\\begin{equation*}\n",
    "y^T = x^T \\begin{bmatrix}\n",
    "\\rvert & \\rvert\t&       & \\rvert \\\\\n",
    "a_1 & a_2 & \\dots & a_n \\\\\n",
    "\\rvert & \\rvert\t&       & \\rvert \n",
    "\\end{bmatrix} = \\begin{bmatrix} x^Ta_1 & x^Ta_2 & \\dots & x^Ta_n \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Cette forme démontre que le $i^{eme}$ élément de $y^T$ est égal au produit scalaire de $x$ et de la $i^{eme}$ colonne de $A$.\n",
    "\n",
    "- Exprimons $A$ selon ses lignes:\n",
    "\n",
    "\\begin{align}\n",
    "y^T &= \\begin{bmatrix} x_1 & x_2 & \\dots & x_n \\end{bmatrix} \\begin{bmatrix}\n",
    "- & a_1^T & - \\\\\n",
    "- & a_2^T & - \\\\\n",
    "  & \\vdots & \\\\\n",
    "- & a_m^T & - \n",
    "\\end{bmatrix} \\\\\n",
    " &= x_1 \\begin{bmatrix} - & a_1^T & - \\end{bmatrix} +\n",
    " x_2 \\begin{bmatrix} - & a_2^T & - \\end{bmatrix} +\n",
    " \\dots +\n",
    " x_n \\begin{bmatrix} - & a_n^T & - \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Nous voyons ici que $y^T$ est une combinaison linéaire des lignes de $A$, où les coefficients de la combinaison linéaire sont donnés par les éléments de $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le produit de la matrice A et de x est:\n",
      " y=  [[56]\n",
      " [95]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.matrix([[2,3,4],[5,6,6]])\n",
    "x = np.matrix([1,6,9]).transpose()\n",
    "y = A*x\n",
    "print (\"Le produit de la matrice A et de x est:\\n y= \",y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produit de deux matrices\n",
    "\n",
    "Armés de nos nouvelles connaissances, nous pouvons voir 4 différentes (mais bien etendu équivalentes) façons de comprendre la multiplication matrice à matrice $C = AB$ comme défini dans le début de ce chapitre. Premièrement nous pouvons voir la multiplication matrice à matrice comme un ensemble de produits de vecteurs. Le point de vue le plus évidents, qui découle immédiatement de la définition, est que l'élément $i,j$ de $C$ est égal au produit scalaire de la $i^{eme}$ ligne de $A$ et de la $j^{eme} colonne de $B$. Ceci peut être vu de la manière suivante:\n",
    "\n",
    "\\begin{align}\n",
    "C & = AB \\\\\n",
    "    & = \\begin{bmatrix} \n",
    "            - & a_1^T & - \\\\ \n",
    "            - & a_2^T & - \\\\ \n",
    "              & \\vdots & \\\\\n",
    "            - & a_m^T & - \n",
    "        \\end{bmatrix} \\begin{bmatrix}\n",
    "            \\rvert & \\rvert &       & \\rvert \\\\\n",
    "            b_1    & b_2    & \\dots & b_p    \\\\\n",
    "            \\rvert & \\rvert &       & \\rvert \n",
    "        \\end{bmatrix} \\\\\n",
    "    & = \\begin{bmatrix} \n",
    "        a_1^Tb_1 & a_1^Tb2 & \\dots & a_1^Tb_p \\\\\n",
    "        a_2^Tb_1 & a_2^Tb2 & \\dots & a_2^Tb_p \\\\\n",
    "        \\vdots   & \\vdots  & \\ddots & \\vdots \\\\\n",
    "        a_m^Tb_1 & a_m^Tb2 & \\dots & a_m^Tb_p \n",
    "        \\end{bmatrix} \n",
    "\\end{align}\n",
    "\n",
    "Remarquez que comme $A \\in \\mathbb{R}^{m \\times n}$ et $B \\in \\mathbb{R}^{n \\times p}$, $a_i \\in \\mathbb{R}^n$ et $b_j \\in \\mathbb{R}^n$, tous le produits scalaires ont du sens. Cette représentation de la multiplication est la plus naturelle. Il est cependant possible de représenter $A$ sous forme de colonnes et $B$ par lignes, ce qui nous emmène à l'interprétation de $AB$ comme la somme de produits externes.\n",
    "\n",
    "\\begin{align}\n",
    "    C &= AB  \\\\\n",
    "      &= \\begin{bmatrix}\n",
    "           \\rvert & \\rvert &       & \\rvert \\\\\n",
    "           a_1    & a_2    & \\dots & a_n    \\\\\n",
    "           \\rvert & \\rvert &       & \\rvert \n",
    "         \\end{bmatrix} \\begin{bmatrix}\n",
    "           - & b_1^T & - \\\\\n",
    "           - & b_2^T & - \\\\\n",
    "             & \\vdots & \\\\\n",
    "           - & b_n^T & -  \n",
    "         \\end{bmatrix} \\\\\n",
    "      &=\\sum_{i = 1}^n a_{i}b_{i}^T\n",
    "\\end{align}\n",
    "\n",
    "Dit d'une autre manière, $AB$ est égal à la somme sur $i$ des produits externes de la $i^{eme}$ colonne de $A$ et de la $i^{eme}$ ligne de $B$. Comme, dans ce cas, $a_i \\in  \\mathbb{R}^m$ et $b_i \\mathbb{R}^p$, la dimension du produit externe $a_ib_i^T$ est $m \\times p$, ce qui coïncide avec la dimension de $C$.\n",
    "\n",
    "Nous pouvons voir la multiplication de matrices comme un ensemble de multiplications de matrices à vecteurs. Si nous représentons $B$ en colonnes, nous pouvons voir les colonnes de $C$ comme un produit matrice-vecteur entre $A$ et les colonnes de $B$.\n",
    "\n",
    "\\begin{align}\n",
    "    C &= AB \\\\\n",
    "      &= A \\begin{bmatrix}\n",
    "               \\rvert & \\rvert &       & \\rvert \\\\\n",
    "               b_1    & b_2    & \\dots & b_p    \\\\\n",
    "               \\rvert & \\rvert &       & \\rvert \n",
    "           \\end{bmatrix} \\\\\n",
    "      &= \\begin{bmatrix}\n",
    "               \\rvert & \\rvert &       & \\rvert \\\\\n",
    "               Ab_1    & Ab_2    & \\dots & Ab_p    \\\\\n",
    "               \\rvert & \\rvert &       & \\rvert \n",
    "         \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Ici les $i^{eme}$ colonne de $C$ est donné par le produit matrice à vecteur de droite, $c_i = Ab_i$. De la même manière, nous pouvons représenter $A$ en lignes:\n",
    "\n",
    "\\begin{align}\n",
    "    C &= AB \\\\\n",
    "      &=  \\begin{bmatrix}\n",
    "          - & a_1^T & - \\\\\n",
    "          - & a_2^T & - \\\\\n",
    "          - & \\vdots & - \\\\\n",
    "          - & a_m^T & -\n",
    "           \\end{bmatrix} B \\\\\n",
    "      &=  \\begin{bmatrix}\n",
    "          - & a_1^TB & - \\\\\n",
    "          - & a_2^TB & - \\\\\n",
    "          - & \\vdots & - \\\\\n",
    "          - & a_m^TB & -\n",
    "           \\end{bmatrix} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Dans ce cas la $i^{eme}$ ligne de $C$ est donnée par le produit matrice à vecteur avec le vecteur à gauche, $c_i^T = a_i^TB$.\n",
    "\n",
    "Quelques propriétés utiles de la multiplication matricielle:\n",
    "\n",
    "- La multiplication de matrice est associative: $(AB)C = A(BC)$\n",
    "- La multiplication de matrice est distributive: $A(B+C) = AB+AC$\n",
    "- La multiplication de matrice n'est en **général** pas commutative: $AB \\ne BA$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le produit de la matrice A et de B est:\n",
      " C=  [[ 17  71]\n",
      " [ 35 132]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.matrix([[2,3,4],[5,6,6]])\n",
    "B = np.matrix([[1,6],[5,9],[0,8]])\n",
    "C = A*B\n",
    "print (\"Le produit de la matrice A et de B est:\\n C= \",C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opérations et propriétés\n",
    "\n",
    "Dans ce chapitre sont pésentés différentes opérations et propriétés des matrices et des vecteurs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La matrice identité et les matrices diagonales\n",
    "\n",
    "La matrice **identité**, notée $I \\in \\mathbb{R}^{n \\times n}$, est une matrice carrée ne contenant que des 1 sur la diagonales et des 0 partout ailleurs. C'est à dire:\n",
    "\n",
    "\\begin{align}\n",
    "I = \n",
    "\\begin{cases}\n",
    "1 & \\text{ si } i = j \\\\\n",
    "0 & \\text{ si } i \\ne j\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Elle a pour propriété que pour tout $A \\in \\mathbb{R}^{m \\times n}$,\n",
    "\n",
    "\\begin{align}\n",
    "AI = A = IA\n",
    "\\end{align}\n",
    "\n",
    "Où la dimension de $I$ est déterminée par la dimension de $A$ de manière à ce que la mutiplication soit possible.\n",
    "\n",
    "Une **matrice diagonale** est une matrice pour laquelle tous les éléments qui ne sont pas sur la diagonale sont 0. Elle est généralement noté $D = diag(d_1,d_2,\\dots,d_n)$, avec: \n",
    "\n",
    "\\begin{align}\n",
    "D_ij = \n",
    "\\begin{cases}\n",
    "d_i & \\text{ si } i = j \\\\\n",
    "0 & \\text{ si } i \\ne j\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "On a évidemment: $I = diag(1,1,\\dots,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La matrice identité de taille 3 est:\n",
      " I=  [[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "I = np.identity(3)\n",
    "print (\"La matrice identité de taille 3 est:\\n I= \",I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La transposée\n",
    "\n",
    "La **transposée** d'une matrice s'obtient en inversant les lignes et les colonnes de la matrice. Soit un matrice $A \\in \\mathbb{R}^{m \\times n}$, sa transposée, notée $A^T$, est définie comme:\n",
    "\n",
    "\\begin{align}\n",
    "A^T \\in \\mathbb{R}^{n \\times m}, (A^T)_{ij} = A_{ji}\n",
    "\\end{align}\n",
    "\n",
    "Nous avons déjà utilisé la transposée quand nous avons décrit les vecteurs lignes comme la transposée d'un vecteur colonne.\n",
    "\n",
    "Les propriétés suivantes peuvent être facilement vérifiées:\n",
    "\n",
    "- $(A^T)^T = A$\n",
    "- $(AB)^T=B^TA^T$\n",
    "- $(A+B)^T = A^T + B^T$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Propriété 1:\n",
      " A=  [[ 2  3  4]\n",
      " [ 5  6  6]\n",
      " [ 1  9 -1]] \n",
      " ATT= [[ 2  3  4]\n",
      " [ 5  6  6]\n",
      " [ 1  9 -1]]\n",
      "====== Propriété 2:\n",
      " ABT=  [[ 17  35  46]\n",
      " [ 71 132  79]\n",
      " [ 30  65  42]] \n",
      " BTAT= [[ 17  35  46]\n",
      " [ 71 132  79]\n",
      " [ 30  65  42]]\n",
      "====== Propriété 3:\n",
      " AplusBT=  [[ 3 10  1]\n",
      " [ 9 15 17]\n",
      " [11 10  0]] \n",
      " ATplusBT= [[ 3 10  1]\n",
      " [ 9 15 17]\n",
      " [11 10  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.matrix([[2,3,4],[5,6,6],[1,9,-1]])\n",
    "B = np.matrix([[1,6,7],[5,9,4],[0,8,1]])\n",
    "ATT = A.transpose().transpose()\n",
    "print (\"====== Propriété 1:\\n A= \",A,\"\\n ATT=\",ATT)\n",
    "\n",
    "ABT=(A*B).transpose()\n",
    "BTAT=B.transpose() * A.transpose()\n",
    "print (\"====== Propriété 2:\\n ABT= \",ABT,\"\\n BTAT=\",BTAT)\n",
    "\n",
    "AplusBT= (A+B).transpose()\n",
    "ATplusBT= A.transpose()+B.transpose()\n",
    "print (\"====== Propriété 3:\\n AplusBT= \",AplusBT,\"\\n ATplusBT=\",ATplusBT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les matrices symétriques\n",
    "\n",
    "Une matrice carrée $A \\in  \\mathbb{R}^{n \\times n}$ est **symétrique** si $A = A^T$. Elle est **anti-symétrique** si $A = -A^T$. Il est aisé de montrer que pour toute matrice $A \\in  \\mathbb{R}^{n \\times n}$, la matrice $A + A^T$ est symétrique et la matrice $A - A^T$ est anti-symétrique. De cette propriété découle que toute matrice $A \\in  \\mathbb{R}^{n \\times n}$ peut être décomposée comme la somme d'une matrice symétrique et d'une matrice anti-symétrique car:\n",
    "\n",
    "\\begin{align}\n",
    "A = \\frac{1}{2}(A + A^T)+\\frac{1}{2}(A - A^T)\n",
    "\\end{align}\n",
    "\n",
    "La matrice de droite est symétrique, alors que la matrice de gauche est anti-symétrique. Il est courant de noter n'ensemble des matrices symétriques de taille $n$ comme étant $\\mathbb{S}^{n}$. Si $A \\in \\mathbb{S}^{n}$ alors A est une matrice symétrique de dimension $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Propriété 1:\n",
      "AplusAT=\n",
      " [[ 4  8  5]\n",
      " [ 8 12 15]\n",
      " [ 5 15 -2]]\n",
      "====== Propriété 2:\n",
      "AmoinsAT=\n",
      " [[ 0 -2  3]\n",
      " [ 2  0 -3]\n",
      " [-3  3  0]]\n",
      "====== Propriété 3:\\Formule=\n",
      " [[ 2.  3.  4.]\n",
      " [ 5.  6.  6.]\n",
      " [ 1.  9. -1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.matrix([[2,3,4],[5,6,6],[1,9,-1]])\n",
    "\n",
    "AplusAT = A + A.transpose()\n",
    "AmoinsAT = A - A.transpose()\n",
    "print (\"====== Propriété 1:\\nAplusAT=\\n\",AplusAT)\n",
    "print (\"====== Propriété 2:\\nAmoinsAT=\\n\",AmoinsAT)\n",
    "\n",
    "Formule=(1/2)*AplusAT + (1/2)*AmoinsAT\n",
    "print (\"====== Propriété 3:\\Formule=\\n\",Formule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La trace\n",
    "\n",
    "La trace d'une matrice $A \\in \\mathbb{R}^{n \\times n}$, notée $tr(A)$ ou juste $tr A$ est la somme des éléments de la diagonale de la matrice:\n",
    "\n",
    "\\begin{align}\n",
    "tr(A) = \\sum_{i=1}^n A_{ii}\n",
    "\\end{align}\n",
    "\n",
    "Le trace d'une matrice a les propriétés suivantes:\n",
    "\n",
    "- Pour $A \\in \\mathbb{R}^{n \\times n}$, $tr A = tr A^T$\n",
    "- Pour $A,B \\in \\mathbb{R}^{n \\times n}$, $tr(A+B) = tr A + tr B$\n",
    "- Pour $A \\in \\mathbb{R}^{n \\times n}$, $t \\in \\mathbb{R}$, $tr(tA)=t\\text{ } tr A$\n",
    "- Pour $A,B$ tels que $AB$ est carré, $tr AB = tr BA$\n",
    "- Pour $A,B,C$ tels que $ABS$ est carré, $tr ABC = tr BCA = tra CAB$, cette propriété se généralise pour un ensemble quelconque de matrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Propriété 1: trA= 7  trAT= 7\n",
      "====== Propriété 2: trAplusB= 22  trAplustrB= 22\n",
      "====== Propriété 3: trtA= 56  ttrA= 56\n",
      "====== Propriété 4: trAB= 179  trBA= 179\n",
      "====== Propriété 5: trABC= 1688  trBCA= 1688  trCAB= 1688\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.matrix([[2,3,4],[5,6,6],[1,9,-1]])\n",
    "B = np.matrix([[2,6,4],[1,5,6],[2,9,8]])\n",
    "C = np.matrix([[1,2,9],[2,3,1],[9,1,8]])\n",
    "\n",
    "trA = np.trace(A)\n",
    "trAT = np.trace(A.transpose())\n",
    "print (\"====== Propriété 1: trA=\",trA,\" trAT=\",trAT)\n",
    "\n",
    "trAplusB = np.trace(A+B)\n",
    "trAplustrB = np.trace(A) + np.trace(B)\n",
    "print (\"====== Propriété 2: trAplusB=\",trAplusB,\" trAplustrB=\",trAplustrB)\n",
    "t = 8\n",
    "trtA= np.trace(t*A)\n",
    "ttrA= t * np.trace(A)\n",
    "print (\"====== Propriété 3: trtA=\",trtA,\" ttrA=\",ttrA)\n",
    "\n",
    "trAB=np.trace(A*B)\n",
    "trBA=np.trace(B*A)\n",
    "print (\"====== Propriété 4: trAB=\",trAB,\" trBA=\",trBA)\n",
    "\n",
    "trABC=np.trace(A*B*C)\n",
    "trBCA=np.trace(B*C*A)\n",
    "trCAB=np.trace(C*A*B)\n",
    "print (\"====== Propriété 5: trABC=\",trABC,\" trBCA=\",trBCA,\" trCAB=\",trCAB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normes\n",
    "\n",
    "#### Définitions\n",
    "\n",
    "La **norme** d'un vecteur $\\|x\\|$ est une mesure de la \"longueur\" d'un vecteur. Par exemble, nous avons la norme euclidienne courammment utilisée (ou norme $\\ell_2$) :\n",
    "\n",
    "\\begin{align}\n",
    "\\|x\\|_2 = \\sqrt{ \\sum_{i = 1}^n x_i^2 }\n",
    "\\end{align}\n",
    "\n",
    "Remarquez que : $\\|x\\|_2^2= x^Tx$\n",
    "\n",
    "Plus formellement, une norme est une fonction $f: \\mathbb{R}^n \\to \\mathbb{R}$ qui satisfait 4 propriétés:\n",
    "\n",
    "1. Pour tout $x \\in \\mathbb{R}^n$, $f(x) \\geq 0$\n",
    "2. $f(x) = 0$ si et seulement si $x=0$\n",
    "3. Pour tout $x \\in \\mathbb{R}^n$, $t \\in \\mathbb{R}$, $f(tx)=|t| f(x)$\n",
    "4. Pour tout $x,y \\in \\mathbb{R}^n$, $f(x+y) \\leq f(x)+f(y)$\n",
    "\n",
    "Autres exemple de normes:\n",
    "\n",
    "#### Norme $\\ell_1$\n",
    "\n",
    "\\begin{align}\n",
    "\\|x\\|_1 = \\sum_{i=1}^n|x_i|\n",
    "\\end{align}\n",
    "\n",
    "#### Norme $\\ell_\\infty$\n",
    "\n",
    "\\begin{align}\n",
    "\\|x\\|_\\infty = max_i|x_i|\n",
    "\\end{align}\n",
    "\n",
    "#### Généralisation: norme $\\ell_p$\n",
    "\n",
    "En réalité les trois normes présentées précédemment, sont des exemples de norme de la famille des normes $\\ell_p$. Ces normes sont paramétrées par un nombre réel $p \\geq 1$, et définies comme suit:\n",
    "\n",
    "\\begin{align}\n",
    "\\|x\\|_p = \\left(\\sum_{i=1}^n |x_i|^{p}\\right)^\\frac{1}{p}\n",
    "\\end{align}\n",
    "\n",
    "#### Norme de matrices\n",
    "\n",
    "Des normes peuvent aussi être définies pour des matrices, comme par exemple la norme de Frobenius:\n",
    "\n",
    "\\begin{align}\n",
    "\\| A \\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n A_ij^2} = \\sqrt{ tr (A^TA) }\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norme de Frobenius de A:  14.4568322948\n",
      "Norme de L1 de V:  3.74165738677\n",
      "Norme de Linf de V:  3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "A = np.matrix([[2,3,4],[5,6,6],[1,9,-1]])\n",
    "lF = LA.norm(A,'fro')\n",
    "print(\"Norme de Frobenius de A: \",lF)\n",
    "\n",
    "V= np.matrix([1,2,3])\n",
    "print (\"Norme de L1 de V: \",LA.norm(V,2))\n",
    "print (\"Norme de Linf de V: \",LA.norm(V,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indépendance linéaire et rang\n",
    "\n",
    "Un ensemble de vecteurs $\\{x_1,x_2,\\dots,x_n\\}$ est dit (linéairement) **indépendant** si aucun des vecteurs ne peut être représenté comme combinaison linéaire des autres vecteurs. Réciproquement, si un vecteur  peut être représenté comme une combinaison linéaire des autres vecteurs, il est dit (linéairement) **dépendant**. Par exemple si:\n",
    "\n",
    "\\begin{align}\n",
    "x_n=\\sum_{i=1}^{n-1} \\alpha_i x_i\n",
    "\\end{align}\n",
    "\n",
    "pour les coefficients $\\{\\alpha_1,\\dots,\\alpha_{n-1}\\}$ alors $x_n$ est dépendant de $\\{x_1,\\dots,x_{n-1}\\}$; sinon il est indépendant de $\\{x_1,\\dots,x_{n-1}\\}$.\n",
    "\n",
    "Le rang des colonnes d'une matrice $A$ est le plus grand nombre de colonnes de $A$ qui constitue un ensemble linéairement indépendant. De la même manière, le rang des lignes d'une matrice $A$ est le plus grand nombre de lignes de $A$ qui constitue un ensemble linéairement indépendant. En réalité $rangcolonne(A) = rangligne(A)$, cette quantité est donc simplement appelée **rang** de $A$, notée $rang(A)$. Quelques propriétés du rang:\n",
    "\n",
    "- Pour $A \\in \\mathbb{R}^{m \\times n}$, $rang(A) \\leq min(m,n)$. Si $rang(A) = min(m,n)$, alors $A$ est dit **de rang plein**.\n",
    "- Pour $A \\in \\mathbb{R}^{m \\times n}$, $rang(A) = rang(A^T)$.\n",
    "- Pour $A \\in \\mathbb{R}^{m \\times n}$, $B \\in \\mathbb{R}^{n \\times p}$, $rang(AB) \\leq min(rang(A),rang(b))$.\n",
    "- Pour $A,B \\in \\mathbb{R}^{m \\times n}$, $rang(A+B) \\leq rang(A)+rang(B)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rang de A:  3\n",
      "Rang de B:  2\n",
      "Rang de B:  2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import matrix_rank\n",
    "\n",
    "A = np.matrix([[2,3,4],[5,6,6],[1,9,-1]])\n",
    "print (\"Rang de A: \",matrix_rank(A))\n",
    "B = np.matrix([[2,3,4],[5,6,6]])\n",
    "print (\"Rang de B: \",matrix_rank(B))\n",
    "print (\"Rang de B: \",matrix_rank(B.transpose()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'inverse d'une matrice\n",
    "\n",
    "L'**inverse** d'une matrice carrée $A \\in \\mathbb{R}^{m \\times n}$, est notée $A^{-1}$ et est l'unique matrice telle que:\n",
    "\n",
    "\\begin{align}\n",
    "A^{-1}A= I = AA^{-1}\n",
    "\\end{align}\n",
    "\n",
    "Il en suit que $A^{-1}$ peut ne pas exister pour certaines matrices $A$. Nous disons que $A$ est **inversible** ou **non-singulière** si $A^{-1} $ existe,  et **non-inversible** ou **singulière** sinon. \n",
    "\n",
    "$A^{-1}$ existe si et seulement si $A$ est de rang plain. Nous verrons par la suite qu'il existe d'autres conditions nécessaires et suffisantes pour le caractère inversible des matrices. Les propriétés suivantes partent de l'hypothèse que $A,B \\in \\mathbb{R}^{m \\times n}$ sont non singulières.\n",
    "\n",
    "- $(A^{-1})^{-1} = A$\n",
    "- Si $Ax = b$ , nous pouvons multiplier par $A^{-1}$ des deux cés pour obtenir $x = A^{-1}b$. \n",
    "- $(AB)^{-1}=B^{-1}A^{-1}$\n",
    "- $(A^{-1})^T = (A^T)^{-1}$. Pour cette raison cette matrice est souvent notée $A^{-T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La matrice inverse de A est:\n",
      "Ainv=\n",
      " [[-0.86956522  0.56521739 -0.08695652]\n",
      " [ 0.15942029 -0.08695652  0.11594203]\n",
      " [ 0.56521739 -0.2173913  -0.04347826]]\n",
      "La matrice A*Ainv est:\n",
      "A*Ainv=\n",
      " [[  1.00000000e+00  -1.11022302e-16   0.00000000e+00]\n",
      " [  2.22044605e-16   1.00000000e+00  -6.93889390e-17]\n",
      " [  3.33066907e-16  -1.11022302e-16   1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "A = np.matrix([[2,3,4],[5,6,6],[1,9,-1]])\n",
    "Ainv = inv(A)\n",
    "print(\"La matrice inverse de A est:\\nAinv=\\n\",Ainv)\n",
    "print(\"La matrice A*Ainv est:\\nA*Ainv=\\n\",A*Ainv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice othogonale\n",
    "\n",
    "Deux vecteurs sont $x,y \\in \\mathbb{R}^n$ sont **orthogonaux** si $x^Tx=0$. Un vecteur $x \\in \\mathbb{R}$ est **normalisé** si $\\|x\\|_2 = 1$. Une matrice $U \\in \\mathbb{R}^{n times n}$ est dite **orthogonale**  si toutes ses colonnes sont orthogonales entre elles et sont normalisées (les colonnes sont dites **orthonormales**).\n",
    "\n",
    "Il s'en suit immédiatement de la définition de l'orthogonalité et de la normalité que: \n",
    "\n",
    "\\begin{align}\n",
    "U^TU = I = UU^T\n",
    "\\end{align}\n",
    "\n",
    "En d'autres mots, l'inverse d'une matrice orthogonale est sa transposée. Notez que si $U$ n'est pas carrée - i.e., $U \\in \\mathbb{R}^{m \\times n}, n < m$ - mais que ses colonnes sont quand même orthonormales, alors $U^TU = I$ mais $UU^T \\ne I$. En général, nous n'utilisons le terme orthogonal que pour décrire des matrices $U$ carrées.\n",
    "\n",
    "Une autre propriété intéressante des matrices othogonales est que multiplier un vecteur par un matrice orthogonale ne change pas la norme euclidienne du vecteur, i.e.:\n",
    "\n",
    "\\begin{align}\n",
    "\\|Ux\\|_2 = \\|x\\|_2\n",
    "\\end{align}\n",
    "\n",
    "pour tout $x \\in \\mathbb{R}^n, U \\in \\mathbb{R}^{n \\times n}$ orthogonale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sous-espace vectoriel engendré, projection, image, noyau\n",
    "\n",
    "#### Sous espace vectoriel engendré\n",
    "Le **sous-espace vectoriel engendré** d'un ensemble de vecteurs $\\{x_1,x_2,\\dots,x_n\\}$ est l'ensemble des vecteurs qui peuvent être exprimés sous forme d'une combinaison linéaire de $\\{x_1,x_2,\\dots,x_n\\}$. C'est à dire:\n",
    "\n",
    "\\begin{align}\n",
    "Vect(\\{x_1,x_2,\\dots,x_n\\}) = \n",
    "\\left\\lbrace v : v = \\sum_{i=1}^n \\alpha_ix_i, \\text{  }\\alpha_i \\in \\mathbb{R}\\right\\rbrace\n",
    "\\end{align}\n",
    "\n",
    "On peut démontrer que si $\\{x_1,x_2,\\dots,x_n\\}$ est un ensemble de $n$ vecteurs linéairement indépendants, où chaque $x_i \\in\\mathbb{R}^n$, alors $Vect(\\{x_1,x_2,\\dots,x_n\\}) = \\mathbb{R}^n$. En d'autre mots, tout vecteur $v \\in \\mathbb{R}^n$ peut être écrit comme une combinaison des vecteurs $x_1$ à $x_n$. \n",
    "\n",
    "__Note__: le sous-espace vectoriel engendré est appelé \"span\" en anglais et noté $span(\\{x_1,x_2,\\dots,x_n\\})$.\n",
    "\n",
    "#### Projection\n",
    "La **projection** d'un vecteur $y \\in \\mathbb{R}^m$ dans l'espace vectoriel engendré par $\\{x_1,x_2,\\dots,x_n\\}$ (nous considérons que $x_i \\in \\mathbb{R}^m$) est le vecteur $v \\in Vect(\\{x_1,x_2,\\dots,x_n\\})$, tel que $v$ soit aussi proche que possible de $y$, comme mesuré par la norme euclidienne $\\|v-y\\|_2$. On note cette projection: $Proj(y;\\{x_1,x_2,\\dots,x_n\\})$, et plus formellement:\n",
    "\n",
    "\\begin{align}\n",
    "Proj(y;\\{x_1,x_2,\\dots,x_n\\}) = \\text{argmin}_{v \\in Vect(\\{x_1,x_2,\\dots,x_n\\})} \\|y-v\\|_2\n",
    "\\end{align}\n",
    "\n",
    "#### Image\n",
    "L'**image** (parfois appelée espace des colonnes) d'une matrice $A \\in \\mathbb{R}^{m \\times n}$, noté $\\mathcal{Im}(A)$ est le sous-espace vectoriel engendré par les colonnes de $A$. En d'autres mots:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{Im}(A)=\\{v \\in \\mathbb{R}^m: v = Ax, \\text{  } x \\in \\mathbb{R}^n\\}\n",
    "\\end{align}\n",
    "\n",
    "Si nous faisons quelques hypothèses (principalement que $A$ est une matrice pleine et que $n < m$), la projection d'un vecteur $y \\in \\mathbb{R}^m$ dans l'image de $A$ s'exprime de la manière suivante:\n",
    "\n",
    "\\begin{align}\n",
    "Proj(y;A) = argmin_{v \\in \\mathcal{R}(A)}\\|v-y\\|_2 = A(A^TA)^{-1}A^Ty\n",
    "\\end{align}\n",
    "\n",
    "__Note__: l'image est appelée \"Range\" en anglais et est notée $\\mathcal{R}(A)$.\n",
    "\n",
    "#### Noyau (kernel ou nullspace)\n",
    "\n",
    "Le **noyau** d'une matrice $A \\in \\mathbb{R}^{m \\times n}$, noté $\\mathcal{N}(A)$  est l'ensemble des vecteurs égaux à 0 lorsqu'ils sont multipliés par $A$, i.e.:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{N}(A) = \\{ x \\in \\mathbb{R}^n : Ax = 0 \\}\n",
    "\\end{align}\n",
    "\n",
    "Les vecteurs dans $\\mathcal{Im}(A)$ sont de taille $m$, alors que les vecteurs dans $\\mathcal{N}(A)$ sont de taille $n$, les vecteurs dans $\\mathcal{Im}(A^T)$ et dans $\\mathcal{N}(A)$ sont quant-à eux tous dans $\\mathbb{R}^n$. En réalité nous pouvons en dire beaucoup plus, il s'avère que:\n",
    "\n",
    "\\begin{align}\n",
    "\\{ w: w = u + v, u \\in \\mathcal{Im}(A^T), v \\in \\mathcal{N}(A) \\} = \\mathbb{R}^n \\text{ and } \\mathcal{Im}(A^T) \\cap \\mathcal{N}(A) = \\emptyset\t\n",
    "\\end{align}\n",
    "\n",
    "En d'autres mots, $\\mathcal{Im}(A^T)$  et $ \\mathcal{N}(A)$ sont deux sous-espaces disjoint qui convrent entièrement l'espace $\\mathbb{R}^n$. Les ensembles de ce type sont appelés des **compléments orthogonaux**, et ils sont notés  $\\mathcal{Im}(A^T) = \\mathcal{N}(A)^\\perp$\n",
    "\n",
    "__Note__: le noyau est appelée \"nullspace\" en anglais et est notée $\\mathcal{N}(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le déterminant\n",
    "\n",
    "Le **déterminant** d'une matrice carrée $A \\in \\mathbb{R}^{n \\times n}$, est une fonction $det :  \\mathbb{R}^{n \\times n} \\to \\mathbb{R}$, et est noté $\\left\\vert A\\right\\vert$ ou $det(A)$, ou en supprimant les parenthèses $det A$. La formule complète de calcul du déterminant ne donne que peu d'intuition sur son sens, nous allons donc à la place donner trois propriétés définissant le déterminant, desquelles découle le reste (ainsi que la formule générale). \n",
    "\n",
    "1. Le déterminant de la matrice identité est 1: $\\left\\vert I\\right\\vert = 1$.\n",
    "2. Etant donnée une matrice $A \\in \\mathbb{R}^{n \\times n}$, si on multiplie une ligne de $A$ par $t \\in \\mathbb{R}$, alors les déterminant de la nouvelle matrice est $t\\left\\vert A\\right\\vert$.\n",
    "\\begin{equation*}\n",
    "\\left\\vert \\text{ } \\begin{bmatrix}\n",
    "- & a_1^T & - \\\\\n",
    "- & a_2^T & - \\\\\n",
    "  & \\vdots & \\\\\n",
    "- & a_m^T & - \n",
    "\\end{bmatrix} \\text{ } \\right\\vert = t \\left\\vert A \\right\\vert\n",
    "\\end{equation*}\n",
    "3. Si nous échangeons deux lignes $a_i^T$ et $a_j^T$ de $A$, le déterminant de la nouvelle matrice est $-\\left\\vert A\\right\\vert$, par exemple:\n",
    "\\begin{equation*}\n",
    "\\left\\vert \\text{ } \\begin{bmatrix}\n",
    "- & a_2^T & - \\\\\n",
    "- & a_1^T & - \\\\\n",
    "  & \\vdots & \\\\\n",
    "- & a_m^T & - \n",
    "\\end{bmatrix} \\text{ } \\right\\vert =  -t \\left\\vert A \\right\\vert\n",
    "\\end{equation*}\n",
    "\n",
    "Ces propriétés ne nous donnent cependant que peu d'intuition sur la nature du déterminant, nous allons donc ajouter quelques propriétés qui découlent des 3 précédentes.\n",
    "\n",
    "- Pour $A \\in \\mathbb{R}^{n \\times n}$, $|A|=|A^T|$.\n",
    "- Pour $A,B \\in \\mathbb{R}^{n \\times n}$, $|AB|=|A||B|$.\n",
    "- Pour $A \\in \\mathbb{R}^{n \\times n}$, $|A|=0$ si et seulement si $A$ est singulière (non inversible).\n",
    "- Pour $A \\in \\mathbb{R}^{n \\times n}$, et $A$ non-singulière, $|A|^{-1} = \\frac{1}{|A|}$.\n",
    "\n",
    "Avant de donner une définition générale du déterminant, nous définissons, pour $A \\in \\mathbb{R}^{n \\times n}$, $A_{\\setminus i,\\setminus j} \\in \\mathbb{R}^{(n-1) \\times (n-1)}$ comme étant la matrice résultant de la suppression de la $i^{eme}$ ligne et de la $j^{eme}$ colonne de $A$. La formule générale (récursive) du déterminant devient:\n",
    "\n",
    "\\begin{align}\n",
    "|A| &= \\sum_{i=1}^n (-1)^{i+j} a_{ij} \\left\\vert A_{\\setminus i,\\setminus j} \\right\\vert & (\\text{pour tout  } j \\in 1,\\dots,n)\\\\\n",
    " &= \\sum_{j=1}^n (-1)^{i+j} a_{ij} \\left\\vert A_{\\setminus i,\\setminus j} \\right\\vert & (\\text{pour tout  } i \\in 1,\\dots,n)\\\\\n",
    "\\end{align}\n",
    "\n",
    "avec le cas spécial $|A| = a_{11}$ pour $A \\in \\mathbb{R}^{1 \\times 1}$. Si nous procédons à une expansion de cette formule complètement pour $A \\in \\mathbb{R}^{n \\times n}$, il y aurait un total de $n!$ (n factoriel)termes. C'est pour cette raison que nous n'allons pas écrire la formule explicite au-delà de matrices 3x3.\n",
    "\n",
    "\\begin{align}\n",
    "\\left\\vert \\text{ } \\begin{bmatrix}\n",
    "a_{11} \n",
    "\\end{bmatrix} \\text{ } \\right\\vert & = a_{11} \\\\\n",
    "\\left\\vert \\text{ } \\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \n",
    "\\end{bmatrix} \\text{ } \\right\\vert & = a_{11}a_{22} - a_{12}a_{21} \\\\\n",
    "\\left\\vert \\text{ } \\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13}\\\\\n",
    "a_{21} & a_{22} & a_{23}\\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix} \\text{ } \\right\\vert & = \\begin{array} \n",
    "+ a_{11}a_{22}a_{21} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} \\\\\n",
    "- a_{11}a_{23}a_{32} - a_{12}a_{21}a_{33} - a_{13}a_{22}a_{31} \\end{array}\n",
    "\\end{align}\n",
    "\n",
    "La matrice **adjointe**, aussi appelée **comatrice** d'une matrice $A \\in \\mathbb{R}^{n \\times n}$, notée $adj(A)$, est définie comme suit:\n",
    "\n",
    "\\begin{align}\n",
    "adj(A) \\in \\mathbb{R}^{n \\times n}, (adj(A))_{ij}=(-1)^{i + j}|A_{\\setminus j,\\setminus i}|\n",
    "\\end{align}\n",
    "\n",
    "Il peut être démontré que pour toute matrice $A \\in \\mathbb{R}^{n \\times n}$ non singulières:\n",
    "\n",
    "\\begin{align}\n",
    "A^{-1}= \\frac{1}{|A|}adj(A)\n",
    "\\end{align}\n",
    "\n",
    "En pratique:\n",
    "\n",
    "\\begin{align}\n",
    "B &=\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "1 & 1 & 2 \\\\\n",
    "-1 & -4 & -1\n",
    "\\end{bmatrix} \\\\\n",
    "det(B) &= +6 \\\\\n",
    "adj(B) &=\\begin{bmatrix}\n",
    "+ \\begin{vmatrix} 1 & 2 \\\\ -4 & -1 \\end{vmatrix} &\n",
    "- \\begin{vmatrix} 0 & 2 \\\\ -1 & -1 \\end{vmatrix} &\n",
    "+ \\begin{vmatrix} 0 & 1 \\\\ -1 & -4 \\end{vmatrix} \\\\\n",
    "- \\begin{vmatrix} 2 & 3 \\\\ -4 & -1 \\end{vmatrix} &\n",
    "+ \\begin{vmatrix} 1 & 3 \\\\ -1 & -1 \\end{vmatrix} &\n",
    "- \\begin{vmatrix} 1 & 2 \\\\ -1 & -4 \\end{vmatrix} \\\\\n",
    "+ \\begin{vmatrix} 2 & 3 \\\\ 1 & 2 \\end{vmatrix} &\n",
    "- \\begin{vmatrix} 1 & 3 \\\\ 0 & 2 \\end{vmatrix} &\n",
    "+ \\begin{vmatrix} 1 & 2 \\\\ 0 & 1 \\end{vmatrix} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "adj(B) &=\\begin{bmatrix}\n",
    "7 & -2 & 1 \\\\\n",
    "-10 & 2 & 2 \\\\\n",
    "1 & -2 & 1\n",
    "\\end{bmatrix} \\\\\n",
    "adj(B)^T &=\\begin{bmatrix}\n",
    "7 & -10 & 1 \\\\\n",
    "-2 & 2 & -2 \\\\\n",
    "1 & 2 & 1\n",
    "\\end{bmatrix} \\\\\n",
    "\\text{d'où } B^{-1} &= \\frac{1}{6}\\begin{bmatrix}\n",
    "7 & -10 & 1 \\\\\n",
    "-2 & 2 & -2 \\\\\n",
    "1 & 2 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Bien que que cette formule soit élégante, il existe des méthodes bien plus efficaces (numériquement) pour calculer l'inverse d'une matrice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forme quadratique et matrices positives semi définies\n",
    "\n",
    "Etant donnée une matrice carrée $A \\in \\mathbb{R}^{n \\times n}$ et un vecteur $x \\in \\mathbb{R}^n$, la valeur scalaire $x^TAx$ est appelée **la forme quadratique**. En écrivant explicitement cette forme, nous voyons que:\n",
    "\n",
    "\\begin{align}\n",
    "x^TAx = \\sum_{i=1}^n \\sum_{j=1}^n A_{ij}x_ix_j\n",
    "\\end{align}\n",
    "\n",
    "Exemple:\n",
    "\n",
    "\\begin{align}\n",
    "x &= \\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}\\\\\n",
    "A &= \\begin{bmatrix}a_{11} & a_{12}\\\\ a_{21} & a_{22} \\end{bmatrix} \\\\\n",
    "x^TAx &= \\begin{bmatrix}x_1 & x_2\\end{bmatrix}\n",
    "\\begin{bmatrix}a_{11} & a_{12}\\\\ a_{21} & a_{22} \\end{bmatrix} \n",
    "\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}\\\\\n",
    "x^TAx &= \\begin{bmatrix}x_1 & x_2\\end{bmatrix}\n",
    "\\begin{bmatrix}a_{11}x_1 + a_{12}x_2\\\\ a_{21}x1 + a_{22}x_2 \\end{bmatrix}  \\\\\n",
    "x^TAx &= x_1 \\times (a_{11}x_1 + a_{12}x_2)+x_2\\times (a_{21}x_1 + a_{22}x_2) \\\\\n",
    "x^TAx &= a_{11}x_1^2 + (a_{12} + a_{21}) x_1x_2 + a_{22}x_2^2\n",
    "\\end{align}\n",
    "\n",
    "Notez que:\n",
    "\n",
    "\\begin{align}\n",
    "x^TAx = (x^TAx)^T = x^TA^Tx = x^T(\\frac{1}{2}A+\\frac{1}{2}A^T)x\n",
    "\\end{align}\n",
    "\n",
    "i.e., seule la partie symétrique de $A$ contribue à la forme quadratique. C'est pour cette raison que nous partons souvent du principe implicite que les matrices impliquées dans les formes quadratiques sont symétriques.\n",
    "\n",
    "Donnons les définitions suivantes:\n",
    "\n",
    "- Une matrice symétrique $A \\in \\mathbb{S}^{n}$ est **positive  définie** (PD) si pour tous les vecteurs non nuls $x^TAx \\ge 0$. Cette propriété est notée $A \\succ 0$ (ou juste $A > 0$). L'ensemble des matrices PD est souvent notée $\\mathbb{S}_{++}^{n}$\n",
    "- Une matrice symétrique $A \\in \\mathbb{S}^{n}$ est **positive semi définie** (PSD) si pour tous les vecteurs $x^TAx \\ge 0$. Cette propriété est notée $A \\succeq 0$ (ou juste $A \\ge 0$). L'ensemble des matrices PSD est souvent notée $\\mathbb{S}_{+}^{n}$\n",
    "- Une matrice symétrique $A \\in \\mathbb{S}^{n}$ est **negative définie** (ND) si pour tous les vecteurs non nuls $x^TAx < 0$. Cette propriété est notée $A \\prec 0$ (ou juste $A < 0$).\n",
    "- Une matrice symétrique $A \\in \\mathbb{S}^{n}$ est **negative semi définie** (NSD) si pour tous les vecteurs $x^TAx \\le 0$. Cette propriété est notée $A \\preceq 0$ (ou juste $A \\le 0$).\n",
    "- Une matrice symétrique $A \\in \\mathbb{S}^{n}$ est **indéfinie**, si elle n'est ni PSD di NSD - i.e., si il existe $x_1,x_2 \\in \\mathbb{R}^n$ tel que $x_1^TAx_1 > 0$ et $x_2^TAx_2 < 0$ \n",
    "\n",
    "Il est évident que si $A$ est PD, alors $-A$ est ND et vice versa. De la même manière si $A$ est PSD, alors $-A$ est NSD et vice versa. Si $A$ est indéfinie, alors $-A$ est indéfinie. Il peut être montré que les matrices PD et ND sont toujours inversibles.\n",
    "\n",
    "Enfin, il y a un type de matrice positive définie qui apparait fréquemment: soit une matrice $A \\in \\mathbb{R}^{m \\times n}$ (pas nécessairement symétrique ou carré), la matrice $G = A^TA$ (parfois appelée **matrice de Gram** ) est toujours positive et semi définie. Plus encore, si $m \\ge n$ (et nous partons de l'hypothèse que $A$ est de rang plein), alors $G = A^TA$ est positive et définie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valeurs et vecteurs propres\n",
    "\n",
    "Soit une matrice $A \\in \\mathbb{R}^{n \\times n}$, nous disons que $\\lambda \\in \\mathbb{C}$ est une **valeur propre** de $A$ et que $x \\in \\mathbb{C}^n$ est le **vecteur propre** correspondant si:\n",
    "\n",
    "\\begin{align}\n",
    "Ax= \\lambda x, \\text{    } x \\ne 0\n",
    "\\end{align}\n",
    "\n",
    "Intuitivement, cette définition signifie de la multiplication de $A$ par le vecteur $x$ résulte un nouveau vecteur pointant dans la même direction que $x$, mais dilaté d'un facteur $\\lambda$. Notez aussi que pour tout vecteur propre $x \\in \\mathbb{C}^n$ et scalaire $c \\in \\mathbb{C}$, $A(cx) = cAx = c\\lambda x = \\lambda(cx)$, donc $cx$ est aussi un vecteur propre de $A$. C'est pour cela que lorsque l'on parle **du** vecteur propre associé à $\\lambda$, nous partons du principe que le vecteur propre est normalisé de longueur 1.\n",
    "\n",
    "Nous pouvons alors réécrire l'équation précédente pour définir que $(\\lambda,x)$ est un couple de valeur et vecteur propre de $A$:\n",
    "\n",
    "\\begin{align}\n",
    "(\\lambda I - A)x = 0, \\text{  } x \\ne 0\n",
    "\\end{align}\n",
    "\n",
    "Mais $(\\lambda I - A)x = 0$ a un solution non nulle pour $x$ si et seulement si $(\\lambda I - A)$ a un un noyau non null, ce qui est seulement le cas si $(\\lambda I - A)$ est une matrice singulière, i.e.,\n",
    "\n",
    "\\begin{align}\n",
    "\\left\\vert(\\lambda I - A)\\right\\vert = 0\n",
    "\\end{align}\n",
    "\n",
    "Nous pouvons maitenant utiliser la définition du déterminant pour développer cette expression en une (très grande) équation polynomiale en $\\lambda$, de degré maximum $n$. Nous pouvons trouver les $n$ racines de cette équation polynomiale (éventuellement complexes) qui sont les $n$ valeurs propres recherchées $\\lambda_1, \\lambda_2, \\dots, \\lambda_n$. \n",
    "Pour trouver le vecteur propre de la valeur propre $\\lambda_i$, nous n'avons qu'à résoudre l'équation linéaire $(\\lambda_i I - A)x = 0$. Cette méthode n'est cependant pas utilisée en pratique pour le calcul numérique des valeurs propres et des vecteurs propres (l'expension complète du déterminant est en complexité $n!$).\n",
    "\n",
    "La suite définit les propriétés des valeurs propres et des vecteurs propres. Toutes les propriétés partent des hypothèses suivantes: $A \\in \\mathbb{R}^{n \\times n}$, $A$ a comme valeurs propres $\\lambda_1, \\lambda_2, \\dots, \\lambda_n$, et comme vecteurs propres associés $x_1, x_2, \\dots, x_n$.\n",
    "\n",
    "- La trace de $A$ est égale à la somme de ses valeurs propres,\n",
    "\n",
    "\\begin{align}\n",
    "    tr A = \\sum_{i=1}^n \\lambda_i\n",
    "\\end{align}\n",
    "\n",
    "- Le déterminand de $A$ est égal au produit des ses valeurs propres,\n",
    "\n",
    "\\begin{align}\n",
    "    |A| = \\prod_{i=1}^n \\lambda_i\n",
    "\\end{align}\n",
    "\n",
    "- Le rang de $A$ est égal au nombre de valeurs propres non nulles de $A$.\n",
    "- Si $A$ n'est pas singulière, alors $1/\\lambda_i$ est une valeur propre de $A^{-1}$ avec comme vecteur propre associé $x_i$, i.e., $A^{-1}c_i = (1/\\lambda_i)x_i$.\n",
    "- Les valeurs propres d'une matrice diagonale $D= diag(d_1,d_2,\\dots,d_n)$ sont simplement les éléments de la diagonale $d_1,d_2,\\dots,d_n$.\n",
    "\n",
    "Nous pouvons écrire l'équation de tous les vecteurs propres simultanément comme:\n",
    "\n",
    "\\begin{align}\n",
    "    AX = X\\Lambda\n",
    "\\end{align}\n",
    "\n",
    "Où les colonne de $X \\in \\mathbb{R}^{n \\times n}$ sont les vecteurs propres de $A$ et $\\Lambda$ est la matrice diagonale dont les éléments sont les valeurs propres de $A$, i.e.,\n",
    "\n",
    "\\begin{align}\n",
    "    X \\in  \\mathbb{R}^{n \\times n} = \\begin{bmatrix}\n",
    "\\rvert & \\rvert\t&       & \\rvert \\\\\n",
    "x_1 & x_2 & \\dots & x_n \\\\\n",
    "\\rvert & \\rvert\t&       & \\rvert \n",
    "\\end{bmatrix}\\text{ , } \\Lambda = diag(\\lambda_1, \\lambda_2, \\dots, \\lambda_n)\n",
    "\\end{align}\n",
    "\n",
    "Si les vecteurs propres de $A$ sont linéairement indépendants, alors la matrice $X$  est inversible, donc $A = X \\Lambda X^{-1}$. Une matrice qui peut sécrire sous cette forme est dite **diagonalisable**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valeurs et vecteurs propres des matrices symétriques\n",
    "\n",
    "Deux propriété remarquables apparaissent lorsque nous étudions les valeurs propres et les vecteurs propres de matrice symétrique $A \\in \\mathbb{S}^n$. Premièrement, il peut être montré que les valeurs propres de $A$ sont réelles. Deuxièmement, les vecteurs propres forment une base orthonormale, i.e., la matrice $X$ définie précédemment est une matrice orthogonale (pour cette raison, nous notons la matrice des vecteurs propres $U$ dans ce cas). Nous pouvons alors écrire $A$  comme $A = U\\Lambda U^T$ (car l'inverse d'une matrice orthogonale est sa transposée).\n",
    "\n",
    "En utilisant cette définition, on peut montrer que la \"définition\" d'une matrice dépend entièrement du signe de ses valeurs propres: Soit $A \\in \\mathbb{S}^n = U\\Lambda U^T$. Alors:\n",
    "\n",
    "\\begin{align}\n",
    "x^TAx = x^TU\\Lambda U^T x= y^T\\Lambda y = \\sum_{i=1}^n \\lambda_iy_i^2 \\end{align}\n",
    "\n",
    "Où $y = U^Tx$ et comme $U$ est de rang plein, tous les vecteurs $y \\in \\mathbb{R}^{n}$, peuvent s'écrire sous cette forme. Comme $y_i^2$ est toujours positif, alors le signe de cette expression dépend entièrement des $\\lambda_i$.\n",
    "\n",
    "- Si tous les $\\lambda_i > 0$, alors $A$ est de type PD. \n",
    "- Si tous les $\\lambda_i \\ge 0$, alors $A$ est de type PSD. \n",
    "- Si tous les $\\lambda_i < 0$, alors $A$ est de type ND. \n",
    "- Si tous les $\\lambda_i \\le 0$, alors $A$ est de type NSD. \n",
    "- Si $A$ a des valeurs propres positives et négatives, $A$ est indéfinie.\n",
    "\n",
    "Une application fréquente des valeurs et vecteurs propres est la maximisation de fonctions d'une matrice. En particulier, pour une matrice $A \\in \\mathbb{S}^n$, étudions les problême de maximisation suivant:\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{max}_{x \\in \\mathbb{R}^n} x^TAx \\text{ tel que } \\|x\\|_2^2=1\n",
    "\\end{align}\n",
    "\n",
    "i.e., nous voulons trouver les vecteur (de norme 1) qui maximise la forme quadratique. Si nous considérons les valeurs propres ordonnées telles que $\\lambda_1 \\ge \\lambda_1 \\ge \\dots \\ge \\lambda_n$, le vecteur $x$ optimal pour ce probleme d'optimisation est $x_1$. Dans ce cas la valeur maximum de la forme quadratique est $\\lambda_1$.  \n",
    "\n",
    "Par symétrie, la solution optimale pour le problème :\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{min}_{x \\in \\mathbb{R}^n} x^TAx \\text{ tel que } \\|x\\|_2^2=1\n",
    "\\end{align}\n",
    "\n",
    "est le vecteur $x_n$ et a pour valeur minimale $\\lambda_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul matriciel\n",
    "\n",
    "### Le gradient\n",
    "\n",
    "Soit $f : \\mathbb{R}^{m \\times n} \\to \\mathbb{R}$ une fonction prenant en paramètre une matrice $A$ de dimension $m \\times n$ et retournant une valeur réelle. Alors le **gradient** de $f$ (selon $A \\in  \\mathbb{R}^{m \\times n}$) est la matrice des dérivées partielles définie comme:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_A f(A) \\in  \\mathbb{R}^{m \\times n} &= \\begin{bmatrix}\n",
    "\\frac{\\delta f(A)}{\\delta A_{11}} &\n",
    "\\frac{\\delta f(A)}{\\delta A_{12}} &\n",
    "\\dots &\n",
    "\\frac{\\delta f(A)}{\\delta A_{1n}} \\\\\n",
    "\\frac{\\delta f(A)}{\\delta A_{21}} &\n",
    "\\frac{\\delta f(A)}{\\delta A_{22}} &\n",
    "\\dots &\n",
    "\\frac{\\delta f(A)}{\\delta A_{2n}} \\\\\n",
    "\\vdots   & \\vdots  & \\ddots & \\vdots \\\\\n",
    "\\frac{\\delta f(A)}{\\delta A_{m1}} &\n",
    "\\frac{\\delta f(A)}{\\delta A_{m2}} &\n",
    "\\dots &\n",
    "\\frac{\\delta f(A)}{\\delta A_{mn}} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\text{i.e.} \\\\\n",
    "(\\nabla_A f(A))_{ij} &= \\frac{\\delta f(A)}{\\delta A_{ij}}\n",
    "\\end{align}\n",
    "\n",
    "Notez que la dimension de $\\nabla_A f(A)$ est toujours de la même dimension que $A$. Donc si, en particulier, $A$ est juste un vecteur $x \\in \\mathbb{R}^n$,\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_x f(x) &= \\begin{bmatrix}\n",
    "\\frac{\\delta f(x)}{\\delta x_1} \\\\\n",
    "\\frac{\\delta f(x)}{\\delta x_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\delta f(x)}{\\delta x_n}\n",
    "\\end{bmatrix} \n",
    "\\end{align}\n",
    "\n",
    "Il est très important de se souvenir que le gradient d'une fonction est seulement définie si elle retourne une valeur scalaire. Il n'est pas possible par exemple de calculer les gradient de $Ax, A \\in \\mathbb{R}^{n \\times n}$ selon $x$, puisque la fonction retourne un vecteur.\n",
    "\n",
    "Il s'en suit que:\n",
    "\n",
    "- $\\nabla_x(f(x)+g(x) =  \\nabla_x f(x) + \\nabla_x g(x)$  \n",
    "- $\\nabla_x(f(x)g(x) =  f(x) \\nabla_x g(x) + g(x) \\nabla_x f(x)$  \n",
    "- Pour $t \\in \\mathbb{R}, \\nabla_x(t f(x)) = t\\nabla_x(f(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le Hessien\n",
    "\n",
    "Soit $f : \\mathbb{R}^{n} \\to \\mathbb{R}$ une fonction prenant en paramètre un vecteur dimension $n$ et retournant une valeur réelle. Alors le **hessien** de $f$ selon $x$ est la matrice de dimension $n \\times n$ notée $\\nabla_x^2f(x)$ ou simplement $H$:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_x^2 f(x) \\in  \\mathbb{R}^{n \\times n} &= \\begin{bmatrix}\n",
    "\\frac{\\delta f(x)}{\\delta x_1^2} &\n",
    "\\frac{\\delta f(x)}{\\delta x_1 \\delta x_2} &\n",
    "\\dots &\n",
    "\\frac{\\delta f(x)}{\\delta x_1 \\delta x_n} \\\\\n",
    "\\frac{\\delta f(x)}{\\delta x_2 \\delta x_1} &\n",
    "\\frac{\\delta f(x)}{\\delta x_2^2} &\n",
    "\\dots &\n",
    "\\frac{\\delta f(x)}{\\delta x_2 \\delta x_n} \\\\\n",
    "\\vdots   & \\vdots  & \\ddots & \\vdots \\\\\n",
    "\\frac{\\delta f(x)}{\\delta x_n \\delta x_1 } &\n",
    "\\frac{\\delta f(x)}{\\delta x_n \\delta x_2} &\n",
    "\\dots &\n",
    "\\frac{\\delta f(x)}{\\delta x_n^2} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\text{i.e.} \\\\\n",
    "(\\nabla_x^2 f(x))_{ij} &= \\frac{\\delta^2 f(x)}{\\delta x_i \\delta x_j}\n",
    "\\end{align}\n",
    "\n",
    "Notez que le Hessien est une matrice symétrique, car:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\delta^2 f(x)}{\\delta x_i \\delta x_j} = \\frac{\\delta^2 f(x)}{\\delta x_j \\delta x_i}\n",
    "\\end{align}\n",
    "\n",
    "Comme pour le gradient, le hessien n'est défini que si $f(x)$ retourne un scalaire.\n",
    "\n",
    "Il est naturel de considérer les gradient, comme analogue à la dérivée pour les fonctions de vecteurs, et le hessien comme la dérivée seconde. Cette intuition est en général correcte, mais il y a quand même une mise en garde à garder à l'esprit.\n",
    "\n",
    "Premièrement, pour des fonctions à une variable de type $f: \\mathbb{R} \\to \\mathbb{R}$, la dérivée seconde est bien la dérivée de la dérivée première (par définition).\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\delta^2 f(x)}{\\delta x^2} = \\frac{\\delta}{\\delta x} \\frac{\\delta}{\\delta x} f(x)\n",
    "\\end{align}\n",
    "\n",
    "Cependant pour une fonction de vecteur, le gradient d'un fonction est un vecteur, et il n'est pas possible de calculer le gradient d'un vecteur. Le hessien n'est donc pas le gradient d'un gradient. C'est \"presque vrai\" dans un sens : si nous regardons le $i^{eme}$ élément du gradient $(\\nabla_x f(x))_i = \\frac{\\delta f(x)}{\\delta x_i}$, et que nous en prenons le gradient selon $x$, nous obtenons:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_x \\frac{\\delta f(x)}{\\delta x_i} = \\begin{bmatrix}\n",
    "\\frac{\\delta^2 f(x)}{\\delta x_i \\delta x_1} \\\\\n",
    "\\frac{\\delta^2 f(x)}{\\delta x_i \\delta x_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\delta^2 f(x)}{\\delta x_i \\delta x_n} \n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Ce résultat est la $i^{eme}$ colonne du hessien. D'où:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_x^2 f(x) = \\begin{bmatrix}\n",
    "\\nabla_x(\\nabla_x f(x))_1 &\n",
    "\\nabla_x(\\nabla_x f(x))_2 &\n",
    "\\dots &\n",
    "\\nabla_x(\\nabla_x f(x))_n \n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Finalement, noter que bien que nous ayons évoqué le gradient d'une matrice $A \\in \\mathbb{R}^n$ et le Hessien d'un vecteur $x \\in \\mathbb{R}$. Il est bien sûr possible de calculer le hessien d'une matrice, mais le hessien consiste à représenter toutes les dérivée partielles ce qui dans le cas d'une matrice s'avère laborieux $\\frac{\\delta^2 f(A)}{\\delta A_{ij}\\delta A_{kl}}$ .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le gradient et le hessien des fonctions linéaires et quadratiques\n",
    "\n",
    "Essayons maintenant de déterminer les gradient et le hessien de quelques fonctions simples.\n",
    "\n",
    "Pour $x \\in \\mathbb{R}^n$, soit $f(x)=b^Tx$ avec $b \\in \\mathbb{R}^n$. Alors:\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = \\sum_{i=1}^n b_ix_i\n",
    "\\end{align}\n",
    "\n",
    "alors:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\delta f(x)}{\\delta x_k} &= \\frac{\\delta}{\\delta x_k}  \\sum_{i=1}^n b_ix_i \\\\\n",
    " &= b_k\n",
    "\\end{align}\n",
    "\n",
    "De ce résultat, nous pouvons déduire que $\\nabla_xb^Tx = b$. Cette situation est comparable au calcul d'une seule variable, où $\\frac{\\delta}{\\delta x}ax=a$.\n",
    "\n",
    "Considérons maintenant la fonction quadratique $f(x) = x^TAx$ pour $A \\in \\mathbb{S}^n$. Souvenez vous que:\n",
    "\n",
    "\\begin{align}\n",
    "    f(x)=\\sum_{i=1}^n \\sum_{j=1}^n A_{ij}x_ix_j\n",
    "\\end{align}\n",
    "\n",
    "alors:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\delta f(x)}{\\delta x_k} &= \\frac{\\delta}{\\delta x_k}  \\sum_{i=1}^n \\sum_{j=1}^n A_{ij}x_i x_j \\\\\n",
    " &= \\sum_{i=1}^n A_{ik}x_i + \\sum_{i=1}^n A_{kj}x_j  \\\\\n",
    " &= 2 \\sum_{i=1}^n A_{ik}x_i \n",
    "\\end{align}\n",
    "\n",
    "La dernière égalité est vrai car $A$ est symétrique. Notez que le $k^{eme}$ élément de $\\nabla_x f(x)$ est juste le produit scalaire de la $k^{eme}$ ligne de $A$ et de $x$, d'où: $\\nabla_xx^TAx=2Ax$. Encore un fois, cette équation doit vous rappeler le cas d'une fonction d'une variable: $\\frac{\\delta}{\\delta x}ax^2=2ax$\n",
    "\n",
    "Finalement, étudions les Hessien des fonction quadratiques $f(x) = x^TAx$ (il est évident que le Hessien d'une fonction linéaire $b^T$ est 0). Le Hessien est encore plus simple que déterminer le gradient:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\delta^2 f(x)}{\\delta x_k \\delta x_l} &=\n",
    "\\frac{\\delta^2}{\\delta x_k \\delta x_l} \\sum_{i=1}^n \\sum_{j=1}^n  A_{ij} x_i x_j \\\\\n",
    " &= A_{kl} + A_{lk} \\\\\n",
    " &= 2A_{kl}\n",
    "\\end{align}\n",
    "\n",
    "De là, il doit être clair que $\\nabla_x^2x^TAx=2A$, ce qui devait être attendu et analogue au cas d'une fonction à une variable: $\\frac{\\delta^2}{\\delta x^2}ax^2=2a$.\n",
    "\n",
    "Pour récapituler:\n",
    "\n",
    "- $\\nabla_xb^Tx=b$\n",
    "- $\\nabla_xx^TAx=2Ax$ (Si $A$ est symétrique)\n",
    "- $\\nabla_x^2x^TAx=2A$ (Si $A$ est symétrique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les moindres carrés\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
