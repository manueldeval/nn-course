{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilités\n",
    "\n",
    "\n",
    "## Préambule\n",
    "\n",
    "Ce document est une traduction \"libre\", approximative, et incomplète du cours de Samuel Ieong : [Probability Theory Review for Machine Learning](https://see.stanford.edu/materials/aimlcs229/cs229-prob.pdf), aggrémenté de quelques exemples Python. Les lecteurs sont fortement encouragés à se référer au document d'origine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "De manière générale, la théorie des probabilités est l'étude mathématique de l'incertitude. Il joue un rôle central dans l'apprentissage automatique, car la conception d'algorithmes d'apprentissage repose souvent sur le caractère probabiliste des données. Cet ensemble de notes tente de couvrir la base de la théorie des probabilités."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'espace de probabilité\n",
    "\n",
    "Lorsque nous parlons de probabilité, nous nous référons souvent à la probabilité qu'un événement incertain ait lieu dans la nature. Par exemple, nous parlons de la probabilité de pluie mardi prochain. Par conséquent, afin de discuter formellement de la théorie des probabilités, nous devons d'abord clarifier ce que sont les \"événements possibles\" auxquels nous allons attacher des probabilités.\n",
    "\n",
    "\n",
    "Formellement, un espace de probabilité est défini par le triplet $(\\Omega,\\mathcal{F},P)$, où:\n",
    "\n",
    "- $\\Omega$ est l'espace des résultats possibles (ou espace des résultats),\n",
    "- $\\mathcal{F} \\subseteq 2^\\Omega $ est l'espace des événements (mesurables) (ou l'espace des événements),\n",
    "- $P$ est la mesure de la probabilité (ou distribution de probabilité) qui mappe un événement $E \\in \\mathcal{F}$ en une valeur réelle entre 0 et 1 (pensez à $P$ en tant que fonction).\n",
    "\n",
    "\n",
    "Étant donné l'espace de résultat $\\Omega$, il existe certaines restrictions quant au sous-ensemble de $2^\\Omega$ pour qu'il puisse être considéré comme un espace d'événement $\\mathcal{F}$:\n",
    "\n",
    "- L'événement trivial $\\Omega$ et l'événement vide $\\emptyset$ doit être dans $\\mathcal{F}$. \n",
    "- Si $\\alpha,\\beta \\in \\cal{F}$, alors $\\alpha \\cup \\beta \\in \\cal{F}$.\n",
    "- Si $\\alpha \\in \\cal{F}$, alors, $\\Omega \\ \\alpha \\in \\cal{F}$.\n",
    "\n",
    "**Exemple 1**: _Supposons que nous jetions un dé (à six faces). L'espace des résultats possibles $\\Omega =\\{1, 2, 3, 4, 5, 6\\}$. Nous pouvons décider que les événements d'intérêt sont les lancés de dés pairs et impair. Cet espace d'événement sera donné par $\\cal{F} = \\{\\emptyset, \\{1, 3, 5\\}, \\{2, 4, 6\\}, \\Omega \\}$. Dans notre cas:_\n",
    "- Si $E = \\emptyset$, alors $P(E)=0$\n",
    "- Si $E = \\{1, 3, 5\\}$, alors $P(E)=0.5$\n",
    "- Si $E = \\{2, 4, 6\\}$, alors $P(E)=0.5$\n",
    "- Si $E = \\Omega$, alors $P(E)=1$\n",
    "\n",
    "Etant donné un espace d'évenements $cal{F}$, la mesure de probabilité $P$ doit satisfaire les axiomes suivants:\n",
    "\n",
    "- Pour tout $\\alpha \\in \\mathcal{F}, P(\\alpha) \\ge 0$ (une probabilité ne peut pas être négative).\n",
    "- $P(\\Omega) = 1$.\n",
    "- Pour tout $\\alpha, \\beta \\in \\mathcal{F}$ avec $\\alpha \\cap \\beta = \\emptyset$, alors $P(\\alpha \\cup \\beta) =  P(\\alpha)+P(\\beta)$.\n",
    "\n",
    "** Exemple 2**: _En revenant à notre exemple de dés, supposons que nous prenions l'espace d'événement $\\cal{F} = 2^\\Omega$. De plus, nous définissons une distribution de probabilité $P$ sur $\\cal{F}$ telle que:_\n",
    "\n",
    "\\begin{align}\n",
    "P(\\{1\\}) = P(\\{2\\}) = \\dots = P(\\{6\\}) = \\frac{1}{6}\n",
    "\\end{align}\n",
    "\n",
    "_Alors cette distribution $P$ spécifie complètement la probabilité de tout événement donné\n",
    "(à travers l'axiome d'additivité). Par exemple, la probabilité d'un lancer de dés pair sera:_\n",
    "\n",
    "\\begin{align}\n",
    "P(\\{2,4,6\\}) = P(\\{2\\}) + P(\\{4\\}) + P(\\{6\\}) &= \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6}\\\\\n",
    "&= \\frac{1}{2}\n",
    "\\end{align}\n",
    "\n",
    "_Car tous ces événements sont disjoints. _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables aléatoires\n",
    "\n",
    "Les variables aléatoires jouent un rôle important dans la théorie des probabilités. Le fait le plus important à propos des variables aléatoires est qu'elles ne sont **pas** des variables. Elles sont en fait des fonctions qui font correspondre les résultats (dans l'espace des résultats) aux valeurs réelles. En termes de notation, nous désignons habituellement les variables aléatoires par une lettre majuscule. Voyons voir un exemple.\n",
    "\n",
    "**Exemple 3**: _Encore une fois, considérons le processus de lancé d'un dé. Soit $X$ une variable aléatoire qui dépend du résultat du lancé. Un choix naturel pour $X$ serait de faire correspondre le résultat $i$ à la valeur $i$, c'est-à-dire, faire correspondre l'événement d'un lancé \"un\" à la valeur de 1. Notez que nous aurions pu aussi choisir des correspondances étranges. Par exemple, nous pourrions avoir une variable aléatoire $Y$ qui fait correspondre tous les résultats à $0$, ce qui serait une fonction très peu intéressante, ou bien une variable aléatoire $Z$ qui faire correspondre le résultat $i$ à la valeur de $2^i$ si $i$ est impair et $-i$ si $i$ est pair._\n",
    "\n",
    "Dans un sens, les variables aléatoires nous permettent d'abstraire la notion formelle d'espace d'événement, car nous pouvons définir des variables aléatoires qui capturent les événements appropriés. Par exemple, considérez l'espace événementiel des lancés de dés impairs ou pairs dans l'exemple 1. Nous aurions pu définir une variable aléatoire qui prend la valeur 1 si le résultat $i$ est impair et $0$ sinon. Ces types de variables aléatoires binaires sont très courantes dans la pratique, et sont connues sous le nom de variables indicatrices, car elles indiquent si un certain événement s'est produit. Alors pourquoi\n",
    "avons-nous introduit l'espace des événements? C'est parce que quand on étudie la théorie des probabilités (plus rigoureusement) en utilisant la théorie des mesures, la distinction entre espace de résultat et espace événementiel devient très important.\n",
    "\n",
    "À partir de maintenant, nous parlerons principalement de la probabilité par rapport aux variables aléatoires. Le probabilité qu'une variable aléatoire $X$ prenne la valeur $a$ est notée:\n",
    "\n",
    "\\begin{align}\n",
    "P(X=a) \\text{ ou } P_X(A)\n",
    "\\end{align}\n",
    "\n",
    "On notera la plage d'une variable aléatoire $X$ par $Val(X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions, distributions jointes, distributions marginales\n",
    "\n",
    "Nous parlons souvent de la distribution d'une variable. Cette expression se réfère à la probabilité pour qu'une variable aléatoire prenne certaines valeurs. Par exemple,\n",
    "\n",
    "**Exemple 4**: _Soit la variable aléatoire $X$ définie sur l'espace de résultat $\\Omega$ d'un lancé de dés. Si le dé n'est pas pipé, alors la distribution de $X$ doit être:_\n",
    "\n",
    "\\begin{align}\n",
    "P_X(1) = P_X(2) = \\dots = P_X(6) = \\frac{1}{6}\n",
    "\\end{align}\n",
    "\n",
    "Notez que bien que bien que cet exemple ressemble à celui de l'exemple 2, ils ont une sémantique différente. La distribution de probabilité définie dans l'exemple 2 concerne les événements, alors qu'elle est définie ici pour des variables aléatoires.\n",
    "\n",
    "Nous utiliserons comme notation $P(X)$ pour désigner la distribution de probabilité de la variable aléatoire $X$. Parfois, nous parlons de la distribution de probabilité de plus d'une variable, elles sont appélées des distributions conjointes (la probabilité est déterminée conjointement par tous les variables impliquées). \n",
    "\n",
    "**Exemple 5**: _Soit $X$ une variable aléatoire définie sur l'espace de résultat d'un lancé de dés. Soit $Y$ une variable indicatrice qui prend la valeur 1 si une pièce de monnaie est pile et 0 si elle est face. En supposant que les dés et la pièce sont non pipés, la distribution conjointe de $X$ et $Y$ est donnée par:_\n",
    "\n",
    "|  P   | X=1  | X=2  | X=3  | X=4  | X=5  | X=6  |\n",
    "|------|------|------|------|------|------|------|\n",
    "| Y=0  | 1/12 | 1/12 | 1/12 | 1/12 | 1/12 | 1/12 |\n",
    "| Y=1  | 1/12 | 1/12 | 1/12 | 1/12 | 1/12 | 1/12 |\n",
    "\n",
    "\n",
    "Comme précédemment, nous noterons la probabilité que $X$ prenne la valeur $a$ et $Y$ la valeur $b$ soit en notation longue $P(X = a, Y = b)$, soit par notation courte de $P_{XY} (a, b)$. Nous nous référons à leur distribution conjointe par $P(X, Y)$.\n",
    "\n",
    "Étant donné une distribution conjointe sur les variables aléatoires $X$ et $Y$, on appelle la distribution marginale de $X$ ou de $Y$ la distribution de probabilité d'une seule variable aléatoire. Pour connaître la distribution marginale d'une variable aléatoire, nous additionnons toutes les autres variables aléatoires de la distribution. Formellement, nous voulons dire:\n",
    "\n",
    "\\begin{align}\n",
    "P(X) = \\sum_{b \\in Val(Y)}P(X,Y=b)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions conditionnelles\n",
    "\n",
    "Les distributions conditionnelles sont l'un des outils clés de la théorie des probabilités pour raisonner sur l'incertitude. Elles spécifient la distribution d'une variable aléatoire lorsque la valeur d'une autre variable aléatoire est connue (ou plus généralement, quand un événement est connu comme étant vrai).\n",
    "\n",
    "Formellement, la probabilité conditionnelle de $X$ sachant $Y = b$ est définie comme:\n",
    "\n",
    "\\begin{align}\n",
    "P(X = a|Y=b) = \\frac{P(X=a,Y=b)}{P(Y=b)}\n",
    "\\end{align}\n",
    "\n",
    "Notez que la probabilité conditionnelle n'est pas définie lorsque $Y = b$ vaut 0.\n",
    "\n",
    "**Exemple 6**: _Supposons que nous sachions qu'un lancer de dés était impair, et que nous voulons connaître la probabilité d'un «un». Soit $X$ la variable aléatoire du lancé de dés, et $Y$ une variable indicatrice qui prend la valeur 1 si le lancer de dés est impair, alors nous écrivons la probabilité désirée comme suit:_\n",
    "\n",
    "\\begin{align}\n",
    "P(X = a|Y=1) = \\frac{P(X=1,Y=1)}{P(Y=1)} &= \\frac{1/6}{1/2}\\\\\n",
    "&=\\frac{1}{3}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "L'idée de probabilité conditionnelle s'étend naturellement au cas où la distribution d'une variable aléatoire est conditionnée par plusieurs variables, à savoir:\n",
    "\n",
    "\\begin{align}\n",
    "P(X = a|Y=b,Z=c) = \\frac{P(X=a,Y=b,Z=c)}{P(Y=b,Z=c)}\n",
    "\\end{align}\n",
    "\n",
    "En ce qui concerne les notations, nous écrivons $P (X | Y = b)$ pour désigner la distribution de la variable aléatoire $X$ lorsque $Y = b$. Nous pouvons aussi écrire $P(X| Y)$ pour désigner un ensemble de distributions de $X$, une pour chacune des différentes valeurs que Y peut prendre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indépendance\n",
    "\n",
    "En théorie des probabilités, l'indépendance signifie que la distribution d'une variable aléatoire ne change pas lorsqu'on connait la valeur d'une autre variable aléatoire. Dans l'apprentissage automatique, nous faisons souvent de telles suppositions sur nos données. Par exemple, les échantillons d'apprentissage sont supposés être tirés indépendamment d'un espace sous-jacent; l'étiquette de l'échantillon $i$ est supposée indépendante des caractéristiques de l'échantillon $j$ $(i \\ne j)$.\n",
    "\n",
    "Mathématiquement, une variable aléatoire $X$ est indépendante de $Y$ quand:\n",
    "\n",
    "\\begin{align}\n",
    "P(X) = P(X|Y)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "(Notez que nous avons supprimé les valeurs prises par $X$ et $Y$. Cela signifie que l'instruction est vraie pour toutes les valeurs que $X$ et $Y$ peuvent prendre.)\n",
    "Il est facile de vérifier que si $X$ est indépendant de $Y$, alors $Y$ est également indépendant de $X$. Si $X$ et $Y$ sont indépendants nous notons $X \\perp Y$.\n",
    "Une manière équivalente de le dire est:\n",
    "\n",
    "\\begin{align}\n",
    "P(X,Y) = P(X)P(Y)\n",
    "\\end{align}\n",
    "\n",
    "Parfois, nous parlons également d'indépendance conditionnelle, ce qui signifie que si nous connaissons la valeur d'une variable aléatoire (ou plus généralement, un ensemble de variables aléatoires), alors certaines autres variables aléatoires seront indépendantes les unes des autres. Formellement, nous disons \"$X$ et $Y$ sont conditionnellement indépendants de $Z$\" si:\n",
    "\n",
    "\\begin{align}\n",
    "P(X|Y) = P(X|Y,Z) \n",
    "\\end{align}\n",
    "\n",
    "ou de manière équivalente \n",
    "\n",
    "\\begin{align}\n",
    "P(X,Y|Z) = P(X|Z)P(Y|Z)\n",
    "\\end{align}\n",
    "\n",
    "L'hypothèse de Bayes naïve est un exemple d'indépendance conditionnelle . Cette hypothèse est faite dans le contexte d'un algorithme d'apprentissage pour apprendre à classer les emails entre des spams ou des non-spams. Il suppose que la probabilité d'un mot $x$ apparaissant dans un email est conditionnellement indépendante de la présence d'un autre mot $y$ que l'email soit un spam ou non. Ceci n'est évidemment pas juste, car certains mots viennent presque invariablement par paire. Cependant, il s'avère que faire cette hypothèse simplificatrice ne nuit pas beaucoup à la performance et, en tout cas, nous permet d'apprendre à classer les spams rapidement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Règle de chainage et règle de Bayes\n",
    "\n",
    "Nous présentons maintenant deux règles fondamentales mais importantes pour les manipulations de distributions conjointes et de distributions conditionnelles. Le premier est connu comme la règle de chaînage. \n",
    "\n",
    "**Théorème 1**\n",
    "\n",
    "\\begin{align}\n",
    "P(X_1,x_2,\\dots,X_n) = P(X_1)P(X_2|X_1)P(X_3|X_2,X_1) \\dots P(X_n|X_1,\\dots,X_{n-1})\n",
    "\\end{align}\n",
    "\n",
    "La règle des chaînage est souvent utilisée pour évaluer la probabilité conjointe de certaines variables aléatoires, et elle est particulièrement utile lorsqu'il existe une indépendance (conditionnelle) entre les variables. Notez qu'il y a un choix dans l'ordre dans lequel nous démêlons les variables aléatoires lors de l'application de la règle de chaînage; Choisir le bon ordre peut souvent rendre l'évaluation de la probabilité beaucoup plus facile.\n",
    "\n",
    "La deuxième règle que nous allons introduire est la règle de Bayes. La règle de Bayes nous permet de calculer la probabilité conditionnelle $P (X | Y)$ à partir de $P (Y | X)$, dans un sens en \"inversant\" les conditions. \n",
    "\n",
    "**Théorème 2**\n",
    "\n",
    "\\begin{align}\n",
    "P(X|Y)= \\frac{P(Y|X)P(X)}{P(Y)}\n",
    "\\end{align}\n",
    "\n",
    "Démonstration:\n",
    "\n",
    "\\begin{align}\n",
    "& P(X|Y) = \\frac{P(X,Y)}{P(Y)}\\\\ \n",
    "\\text{ or }  & P(Y|X) = \\frac{P(Y,X)}{P(X)} \\\\\n",
    "\\text{ s'en suit } & P(Y,X) = P(Y|X)P(X) = P(X,Y) \\\\\n",
    "\\text{ enfin } & P(X|Y)= \\frac{P(Y|X)P(X)}{P(Y)}\n",
    "\\end{align}\n",
    "\n",
    "Et rappelez-vous que si $P(Y)$ n'est pas donné, nous pouvons toujours appliquer l'équation:\n",
    "\n",
    "\\begin{align}\n",
    "P(Y)= \\sum_{a \\in Val(X)} P(X=a,Y) = \\sum_{a \\in Val(X)} P(Y|X=a)P(X=a)\n",
    "\\end{align}\n",
    "\n",
    "Cette application est parfois appelée loi de probabilité totale.\n",
    "\n",
    "Étendre la règle de Bayes au cas de plusieurs variables aléatoires peut parfois être difficile. Pour être clair, nous allons donner quelques exemples.\n",
    "\n",
    "**Exemple 7**\n",
    "\n",
    "\\begin{align}\n",
    "P(X,Y|Z)&=\\frac{P(Z|X,Y)P(X,Y)}{P(Z)}    = \\frac{P(Y,Z|X)P(X)}{P(Z)} \\\\\n",
    "P(X|Y,Z)&=\\frac{P(Y|X,Z)P(X,Z)}{P(Y,Z)}  = \\frac{P(Y|X,Z)P(X|Z)P(Z)}{P(Y|Z)P(Z)} = \\frac{P(Y|X,Z)P(X|Z)}{P(Y|Z)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définir la distribution de probabilité\n",
    "\n",
    "Nous avons parlé de probabilité de distribution, mais comment est-elle définie? Au sens large, il y a deux classes de distribution qui demandent apparemment un traitement différent (elles peuvent être unifiées par la théorie de la mesure), à savoir les distributions discrètes et les distributions continues.\n",
    "\n",
    "### Les distributions discrètes : fonction de masse\n",
    "\n",
    "Par distribution discrète, nous voulons dire que la variable aléatoire de la distribution ne peut prendre qu'un nombre fini de valeurs différentes (ou que l'espace des possible est fini). Pour définir une distribution discrète, nous pouvons simplement énumérer les probabilités pour que la variable aléatoire prenne chacune des valeurs possibles. Cette énumération est appelée fonction de masse, car elle divise la masse unitaire (la probabilité totale), et les affecte à chaque valeur différente que la variable aléatoire peut prendre. \n",
    "\n",
    "### Les dictributions continues : fonction de densité de probabilité\n",
    "\n",
    "Par distribution continue, nous entendons que la variale aléatoire de la distribution peut prendre une infinité de valeur différentes (l'espace des possible est infini). Cette situation semble plus complexe que le cas discret, car si nous plaçons une quantité de masse non nulle sur chaque valeur, la masse totale va tendre vers l'infini, ce qui est en contradiction avec la contrainte édictant que la somme de toutes les probabilités doit être unitaire.\n",
    "\n",
    "Pour définir une distribution continue, nous allons utiliser une fonction de densité de probabilité. Une fonction de densité de probabilité , $f$, est une fonction intégrable non-négative telle que:\n",
    "\n",
    "\\begin{align}\n",
    "\\int_{Val(X)} f(x)dx = 1\n",
    "\\end{align}\n",
    "\n",
    "La probabilité qu'une variable aléatoire soit entre le valeur $a$ et $b$ est calculée comme suit:\n",
    "\n",
    "\\begin{align}\n",
    "P(a \\le X \\le b) = \\int_a^b f(x)dx = 1\n",
    "\\end{align}\n",
    "\n",
    "Ceci implique que la probabilité qu'un valeur aléatoire d'une distribution continue prenne une valeur donnée est nulle.\n",
    "\n",
    "** Exemple 8 ** _(Distribution uniforme). Considérons une variable aléatoire $X$ uniformément distribuée dans l'intervalle $[0,1]$. La fonction de densité de probabilité est donc:_\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "1 & \\text{ si } 0 \\le x \\le 1\\\\\n",
    "0 & \\text{ sinon }\\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "_Nous pouvons vérifier que $\\int_0^1 1 dx$ vaut en effet 1, et que $f$ est donc une fonction de densité de probabilité. Pour calculer la probabilité que $X$ soit inférieur à $1/2$, nous effectuons :_\n",
    "\n",
    "\\begin{align}\n",
    "P(X \\le 1/2) = \\int_0^{1/2} 1 dx = [x]_0^{1/2} = 1/2\n",
    "\\end{align}\n",
    "\n",
    "_Plus généralement, soit $X$ une distribution uniforme sur l'intervalle $[a,b]$, sa fonction de densité de probabilité est :_\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "\\frac{1}{b-a} & \\text{ si } a \\le x \\le b\\\\\n",
    "0 & \\text{ sinon }\\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Parfois nous parlons de _distribution de probabilité cumulative_. C'est une fonction qui donne la probabilité qu'une variable aléatoire soit plus petite qu'un valeur donnée. Une distribution cumulative $F$ est associée à une fonction de densité de probabilité sous-jascents $f$ de la manière suivante:\n",
    "\n",
    "\\begin{align}\n",
    "F(b) = P(X \\le b) = \\int_{-\\infty}^b f(x) dx \n",
    "\\end{align}\n",
    "\n",
    "Pour étendre la définition de distribution continue à la distribution jointe, la fonction de densité de probabilité est étendue pour prendre plusieurs arguments:\n",
    "\n",
    "\\begin{align}\n",
    "P(a_1 \\le X_1 \\le b_1, \n",
    "  a_2 \\le X_2 \\le b_2,\n",
    "  \\dots,\n",
    "  a_n \\le X_n \\le b_n\n",
    "  ) = \\\\\n",
    "  \\int_{a_1}^{b_1}\n",
    "  \\int_{a_2}^{b_2}\n",
    "  \\dots\n",
    "  \\int_{a_n}^{b_n}f(x_1,x_2,\\dots,x_n) dx_1 dx_2 \\dots dx_n\n",
    "\\end{align}\n",
    "\n",
    "Pour définir la distribution conditionnelle dans le cas de variables aléatoires continues, considérons $f(x,y)$ la distribution joint de $X$ et $Y$, nous avons alors $f(x|y)$ définie de la manière suivante:\n",
    "\n",
    "\\begin{align}\n",
    "f(x|y)=\\frac{f(x,y)}{f(x)}\n",
    "\\end{align}\n",
    "\n",
    "Par exemple:\n",
    "\n",
    "\\begin{align}\n",
    "P(a \\le X \\le b | X = c) = \\int_a^b f(y|c)dy = \\int_a^b \\frac{f(c,y)}{f(c)}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Espérance et variance\n",
    "\n",
    "### Espérance\n",
    "\n",
    "Une des opérations les plus courantes effectuée sur les variables aléatoires est de calculer son espérance, aussi appelée _moyenne_, _moment de premier ordre_, ou _valeur espérée_. L'espérance d'une variable aléatoire, notée $E(X)$, est données par:\n",
    "\n",
    "\\begin{align}\n",
    "            & E(X) = \\sum_{a \\in Val(X)} aP(x =a) \\\\\n",
    "\\text{ ou } & E(X)= \\int_{a \\in Val(X)} xf(x)dx\n",
    "\\end{align}\n",
    "\n",
    "** Exemple 9 ** _Soit X le résultat d'un lancement de dé non pipé. L'espérance de X est:_\n",
    "\n",
    "\\begin{align}\n",
    "E(X) = (1)\\frac{1}{6} + (1)\\frac{1}{6} + \\dots + (6)\\frac{1}{6} = 3 + \\frac{1}{2}  \n",
    "\\end{align}\n",
    "\n",
    "Nous sommes parfois intéressé par l'espérance d'une fonction $f$ d'une variable aléatoire $X$. Souvenez-vous, qu'une variable aléatoire est elle-même une fonction, donc la manière la plus simple est de définir une nouvelle variable aléatoire $Y=f(X)$, et de calculer l'espérance de $Y$ à la place.\n",
    "\n",
    "Si une variable aléatoire est indicatrice (ne peut prendre que la valeur 0 ou 1), son espérance vaut:\n",
    "\n",
    "\\begin{align}\n",
    "E(X) = P(X = 1) \\text{ si X est une variable indicatrice} \n",
    "\\end{align}\n",
    "\n",
    "Quand nous avons une somme de variable aléatoires, une des règles les plus importante est _la linéarité des espérances_.\n",
    "\n",
    "** Théorème 3 ** (Linéarité des espérances). Soit $X_1,X_2,\\dots,X_n$ des variables aléatoire (éventuellement dépendantes):\n",
    "\n",
    "\\begin{align}\n",
    "E(\\sum_n X_n) = \\sum_n E(X_n) \n",
    "\\end{align}\n",
    "\n",
    "Ce théorème est très puissant car il ne dépend pas du caractère dépedant ou non des variables aléatoires. Dans le cas de produit de variables aléatoires, cependant, il y a peu de chose à dire dans le cas général. La seule chose que nous puissions dire est dans le cas de variables aléatoire indépendantes;\n",
    "\n",
    "** Théorème 4 ** Soit $X$ et $Y$ des variables aléatoires indépendantes:\n",
    "\n",
    "\\begin{align}\n",
    "E(XY) = E(X)E(Y) \n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance\n",
    "\n",
    "La _variance_ d'une distribution est la mesure de la \"dispersion\" d'une distribution. Elle est parfois appelée _moment de second ordre_. La variance est définie comme suit:\n",
    "\n",
    "\\begin{align}\n",
    "Var(X) = E((X -E(X))^2)\n",
    "\\end{align}\n",
    "\n",
    "Intuitivement, c'est donc une mesure de la moyenne des écarts à la moyenne. La variance d'une variable aléatoire est souvent notée $\\sigma^2$. La raison de la puissance de 2 est que nous voulons souvent trouver la valeur $\\sigma$, connue sous le nom _d'écart type_. La variance est l'écart type ont donc comme relation évidente : $\\sigma = \\sqrt{Var(X)}$.\n",
    "\n",
    "Pour trouver la variance d'une variable aléatoire $X$, il est souvent plus simple de calculer la valeur suivante (Théorème de Köning-Huygens):\n",
    "\n",
    "\\begin{align} \n",
    "Var(X) = E(X^2) - (E(X))^2\n",
    "\\end{align}\n",
    "\n",
    "** Démonstration **\n",
    "\n",
    "\\begin{align} \n",
    "E(X^2) - (E(X))^2 &= E( X^2 -2XE(X) + E(X)^2 ) ) \\\\\n",
    " &= E(X^2) - E(2XE(X)) + E(E(X)^2) \\\\\n",
    " &= E(X^2)- 2E(X)E(X) + E(X)^2 \\\\\n",
    " &= E(X^2) - E(X)^2 \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Notez que contrairement à l'espérance, la variance n'est pas une fonction linéaire d'un variable aléatoire $X$. En effet, nous pouvons vérifier que la variance de $(aX+b)$ est:\n",
    "\n",
    "\\begin{align}\n",
    "Var(aX+b) = a^2 Var(X) \n",
    "\\end{align}\n",
    "\n",
    "** Démonstration **\n",
    "\\begin{align}\n",
    "Var(aX+b) &= E((aX+b)^2)-(E(aX+b))^2 \\\\\n",
    "&= E(a^2X^2 + b^2 + 2abX)   - ( aE(X) + b )^2  \\\\\n",
    "&= a^2 E(X^2) + b^2 + 2ab E(X) - a^2 E(X)^2 - 2abE(X) - b^2 \\\\\n",
    "&= a^2 (E(X^2) - E(X)^2) \\\\\n",
    "&= a^2 Var(X) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Si les variables aléatoires $X$ et $Y$ sont indépendantes, alors:\n",
    "\\begin{align}\n",
    "Var(X+Y)=Var(X)+Var(Y) & \\text{ si } X \\text{ et } Y \\text{ sont indépendant }\n",
    "\\end{align}\n",
    "\n",
    "Parfois nous parlons aussi de _covariance_ de deux variables aléatoires. C'est un mesure de \"distance\" de deux variables aléatoires. Sa définition est la suivante:\n",
    "\n",
    "\\begin{align}\n",
    "Cov(X,Y) &= E( (X - E(X))(Y - E(Y)) ) \\\\\n",
    "&= E(XY) - E(X)E(Y)\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
