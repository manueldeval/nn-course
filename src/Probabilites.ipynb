{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilités\n",
    "\n",
    "\n",
    "## Préambule\n",
    "\n",
    "Ce document est une traduction \"libre\", approximative, et incomplète du cours de Samuel Ieong : [Probability Theory Review for Machine Learning](https://see.stanford.edu/materials/aimlcs229/cs229-prob.pdf), aggrémenté de quelques exemples Python. Les lecteurs sont fortement encouragés à se référer au document d'origine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "De manière générale, la théorie des probabilités est l'étude mathématique de l'incertitude. Il joue un rôle central dans l'apprentissage automatique, car la conception d'algorithmes d'apprentissage repose souvent sur le caractère probabiliste des données. Cet ensemble de notes tente de couvrir la base de la théorie des probabilités."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'espace de probabilité\n",
    "\n",
    "Lorsque nous parlons de probabilité, nous nous référons souvent à la probabilité qu'un événement incertain ait lieu dans la nature. Par exemple, nous parlons de la probabilité de pluie mardi prochain. Par conséquent, afin de discuter formellement de la théorie des probabilités, nous devons d'abord clarifier ce que sont les \"événements possibles\" auxquels nous allons attacher des probabilités.\n",
    "\n",
    "\n",
    "Formellement, un espace de probabilité est défini par le triplet $(\\Omega,\\mathcal{F},P)$, où:\n",
    "\n",
    "- $\\Omega$ est l'espace des résultats possibles (ou espace des résultats),\n",
    "- $\\mathcal{F} \\subseteq 2^\\Omega $ est l'espace des événements (mesurables) (ou l'espace des événements),\n",
    "- $P$ est la mesure de la probabilité (ou distribution de probabilité) qui mappe un événement $E \\in \\mathcal{F}$ en une valeur réelle entre 0 et 1 (pensez à $P$ en tant que fonction).\n",
    "\n",
    "\n",
    "Étant donné l'espace de résultat $\\Omega$, il existe certaines restrictions quant au sous-ensemble de $2^\\Omega$ pour qu'il puisse être considéré comme un espace d'événement $\\mathcal{F}$:\n",
    "\n",
    "- L'événement trivial $\\Omega$ et l'événement vide $\\emptyset$ doit être dans $\\mathcal{F}$. \n",
    "- Si $\\alpha,\\beta \\in \\cal{F}$, alors $\\alpha \\cup \\beta \\in \\cal{F}$.\n",
    "- Si $\\alpha \\in \\cal{F}$, alors, $\\Omega \\ \\alpha \\in \\cal{F}$.\n",
    "\n",
    "**Exemple 1**: Supposons que nous jetions un dé (à six faces). L'espace des résultats possibles $\\Omega =\\{1, 2, 3, 4, 5, 6\\}$. Nous pouvons décider que les événements d'intérêt sont les lancés de dés pairs et impair. Cet espace d'événement sera donné par $\\cal{F} = \\{\\emptyset, \\{1, 3, 5\\}, \\{2, 4, 6\\}, \\Omega \\}$. Dans notre cas:\n",
    "- Si $E = \\emptyset$, alors $P(E)=0$\n",
    "- Si $E = \\{1, 3, 5\\}$, alors $P(E)=0.5$\n",
    "- Si $E = \\{2, 4, 6\\}$, alors $P(E)=0.5$\n",
    "- Si $E = \\Omega$, alors $P(E)=1$\n",
    "\n",
    "Etant donné un espace d'évenements $cal{F}$, la mesure de probabilité $P$ doit satisfaire les axiomes suivants:\n",
    "\n",
    "- Pour tout $\\alpha \\in \\mathcal{F}, P(\\alpha) \\ge 0$ (une probabilité ne peut pas être négative).\n",
    "- $P(\\Omega) = 1$.\n",
    "- Pour tout $\\alpha, \\beta \\in \\mathcal{F}$ avec $\\alpha \\cap \\beta = \\emptyset$, alors $P(\\alpha \\cup \\beta) =  P(\\alpha)+P(\\beta)$.\n",
    "\n",
    "** Exemple 2**: En revenant à notre exemple de dés, supposons que nous prenions l'espace d'événement $\\cal{F} = 2^\\Omega$. De plus, nous définissons une distribution de probabilité $P$ sur $\\cal{F}$ telle que:\n",
    "\n",
    "\\begin{align}\n",
    "P(\\{1\\}) = P(\\{2\\}) = \\dots = P(\\{6\\}) = \\frac{1}{6}\n",
    "\\end{align}\n",
    "\n",
    "Alors cette distribution $P$ spécifie complètement la probabilité de tout événement donné\n",
    "(à travers l'axiome d'additivité). Par exemple, la probabilité d'un lancer de dés pair sera:\n",
    "\n",
    "\\begin{align}\n",
    "P(\\{2,4,6\\}) = P(\\{2\\}) + P(\\{4\\}) + P(\\{6\\}) &= \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6}\\\\\n",
    "&= \\frac{1}{2}\n",
    "\\end{align}\n",
    "\n",
    "Car tous ces événements sont disjoints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables aléatoires\n",
    "\n",
    "Les variables aléatoires jouent un rôle important dans la théorie des probabilités. Le fait le plus important à propos des variables aléatoires est qu'elles ne sont **pas** des variables. Elles sont en fait des fonctions qui font correspondre les résultats (dans l'espace des résultats) aux valeurs réelles. En termes de notation, nous désignons habituellement les variables aléatoires par une lettre majuscule. Voyons voir un exemple.\n",
    "\n",
    "**Exemple 3**: Encore une fois, considérons le processus de lancé d'un dé. Soit $X$ une variable aléatoire qui dépend du résultat du lancé. Un choix naturel pour $X$ serait de faire correspondre le résultat $i$ à la valeur $i$, c'est-à-dire, faire correspondre l'événement d'un lancé \"un\" à la valeur de 1. Notez que nous aurions pu aussi choisir des correspondances étranges. Par exemple, nous pourrions avoir une variable aléatoire $Y$ qui fait correspondre tous les résultats à $0$, ce qui serait une fonction très peu intéressante, ou bien une variable aléatoire $Z$ qui faire correspondre le résultat $i$ à la valeur de $2^i$ si $i$ est impair et $-i$ si $i$ est pair.\n",
    "\n",
    "Dans un sens, les variables aléatoires nous permettent d'abstraire la notion formelle d'espace d'événement, car nous pouvons définir des variables aléatoires qui capturent les événements appropriés. Par exemple, considérez l'espace événementiel des lancés de dés impairs ou pairs dans l'exemple 1. Nous aurions pu définir une variable aléatoire qui prend la valeur 1 si le résultat $i$ est impair et $0$ sinon. Ces types de variables aléatoires binaires sont très courantes dans la pratique, et sont connues sous le nom de variables indicatrices, car elles indiquent si un certain événement s'est produit. Alors pourquoi\n",
    "avons-nous introduit l'espace des événements? C'est parce que quand on étudie la théorie des probabilités (plus rigoureusement) en utilisant la théorie des mesures, la distinction entre espace de résultat et espace événementiel devient très important.\n",
    "\n",
    "À partir de maintenant, nous parlerons principalement de la probabilité par rapport aux variables aléatoires. Le probabilité qu'une variable aléatoire $X$ prenne la valeur $a$ est notée:\n",
    "\n",
    "\\begin{align}\n",
    "P(X=a) \\text{ ou } P_X(A)\n",
    "\\end{align}\n",
    "\n",
    "On notera la plage d'une variable aléatoire $X$ par $Val(X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions, distributions jointes, distributions marginales\n",
    "\n",
    "Nous parlons souvent de la distribution d'une variable. Cette expression se réfère à la probabilité pour qu'une variable aléatoire prenne certaines valeurs. Par exemple,\n",
    "\n",
    "**Exemple 4**: Soit la variable aléatoire $X$ définie sur l'espace de résultat $\\Omega$ d'un lancé de dés. Si le dé n'est pas pipé, alors la distribution de $X$ doit être:\n",
    "\n",
    "\\begin{align}\n",
    "P_X(1) = P_X(2) = \\dots = P_X(6) = \\frac{1}{6}\n",
    "\\end{align}\n",
    "\n",
    "Notez que bien que bien que cet exemple ressemble à celui de l'exemple 2, ils ont une sémantique différente. La distribution de probabilité définie dans l'exemple 2 concerne les événements, alors qu'elle est définie ici pour des variables aléatoires.\n",
    "\n",
    "Nous utiliserons comme notation $P(X)$ pour désigner la distribution de probabilité de la variable aléatoire $X$. Parfois, nous parlons de la distribution de probabilité de plus d'une variable, elles sont appélées des distributions conjointes (la probabilité est déterminée conjointement par tous les variables impliquées). \n",
    "\n",
    "**Exemple 5**: Soit $X$ une variable aléatoire définie sur l'espace de résultat d'un lancé de dés. Soit $Y$ une variable indicatrice qui prend la valeur 1 si une pièce de monnaie est pile et 0 si elle est face. En supposant que les dés et la pièce sont non pipés, la distribution conjointe de $X$ et $Y$ est donnée par:\n",
    "\n",
    "|  P   | X=1  | X=2  | X=3  | X=4  | X=5  | X=6  |\n",
    "|------|------|------|------|------|------|------|\n",
    "| Y=0  | 1/12 | 1/12 | 1/12 | 1/12 | 1/12 | 1/12 |\n",
    "| Y=1  | 1/12 | 1/12 | 1/12 | 1/12 | 1/12 | 1/12 |\n",
    "\n",
    "\n",
    "Comme précédemment, nous noterons la probabilité que $X$ prenne la valeur $a$ et $Y$ la valeur $b$ soit en notation longue $P(X = a, Y = b)$, soit par notation courte de $P_{XY} (a, b)$. Nous nous référons à leur distribution conjointe par $P(X, Y)$.\n",
    "\n",
    "Étant donné une distribution conjointe sur les variables aléatoires $X$ et $Y$, on appelle la distribution marginale de $X$ ou de $Y$ la distribution de probabilité d'une seule variable aléatoire. Pour connaître la distribution marginale d'une variable aléatoire, nous additionnons toutes les autres variables aléatoires de la distribution. Formellement, nous voulons dire:\n",
    "\n",
    "\\begin{align}\n",
    "P(X) = \\sum_{b \\in Val(Y)}P(X,Y=b)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions conditionnelles\n",
    "\n",
    "Les distributions conditionnelles sont l'un des outils clés de la théorie des probabilités pour raisonner sur l'incertitude. Elles spécifient la distribution d'une variable aléatoire lorsque la valeur d'une autre variable aléatoire est connue (ou plus généralement, quand un événement est connu comme étant vrai).\n",
    "\n",
    "Formellement, la probabilité conditionnelle de $X$ sachant $Y = b$ est définie comme:\n",
    "\n",
    "\\begin{align}\n",
    "P(X = a|Y=b) = \\frac{P(X=a,Y=b)}{P(Y=b)}\n",
    "\\end{align}\n",
    "\n",
    "Notez que la probabilité conditionnelle n'est pas définie lorsque $Y = b$ vaut 0.\n",
    "\n",
    "**Exemple 6**: Supposons que nous sachions qu'un lancer de dés était impair, et que nous voulons connaître la probabilité d'un «un». Soit $X$ la variable aléatoire du lancé de dés, et $Y$ une variable indicatrice qui prend la valeur 1 si le lancer de dés est impair, alors nous écrivons la probabilité désirée comme suit:\n",
    "\n",
    "\\begin{align}\n",
    "P(X = a|Y=1) = \\frac{P(X=1,Y=1)}{P(Y=1)} &= \\frac{1/6}{1/2}\\\\\n",
    "&=\\frac{1}{3}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "L'idée de probabilité conditionnelle s'étend naturellement au cas où la distribution d'une variable aléatoire est conditionnée par plusieurs variables, à savoir:\n",
    "\n",
    "\\begin{align}\n",
    "P(X = a|Y=b,Z=c) = \\frac{P(X=a,Y=b,Z=c)}{P(Y=b,Z=c)}\n",
    "\\end{align}\n",
    "\n",
    "En ce qui concerne les notations, nous écrivons $P (X | Y = b)$ pour désigner la distribution de la variable aléatoire $X$ lorsque $Y = b$. Nous pouvons aussi écrire $P(X| Y)$ pour désigner un ensemble de distributions de $X$, une pour chacune des différentes valeurs que Y peut prendre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indépendance\n",
    "\n",
    "En théorie des probabilités, l'indépendance signifie que la distribution d'une variable aléatoire ne change pas lorsqu'on connait la valeur d'une autre variable aléatoire. Dans l'apprentissage automatique, nous faisons souvent de telles suppositions sur nos données. Par exemple, les échantillons d'apprentissage sont supposés être tirés indépendamment d'un espace sous-jacent; l'étiquette de l'échantillon $i$ est supposée indépendante des caractéristiques de l'échantillon $j$ $(i \\ne j)$.\n",
    "\n",
    "Mathématiquement, une variable aléatoire $X$ est indépendante de $Y$ quand:\n",
    "\n",
    "\\begin{align}\n",
    "P(X) = P(X|Y)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "(Notez que nous avons supprimé les valeurs prises par $X$ et $Y$. Cela signifie que l'instruction est vraie pour toutes les valeurs que $X$ et $Y$ peuvent prendre.)\n",
    "Il est facile de vérifier que si $X$ est indépendant de $Y$, alors $Y$ est également indépendant de $X$. Si $X$ et $Y$ sont indépendants nous notons $X \\perp Y$.\n",
    "Une manière équivalente de le dire est:\n",
    "\n",
    "\\begin{align}\n",
    "P(X,Y) = P(X)P(Y)\n",
    "\\end{align}\n",
    "\n",
    "Parfois, nous parlons également d'indépendance conditionnelle, ce qui signifie que si nous connaissons la valeur d'une variable aléatoire (ou plus généralement, un ensemble de variables aléatoires), alors certaines autres variables aléatoires seront indépendantes les unes des autres. Formellement, nous disons \"$X$ et $Y$ sont conditionnellement indépendants de $Z$\" si:\n",
    "\n",
    "\\begin{align}\n",
    "P(X|Y) = P(X|Y,Z) \n",
    "\\end{align}\n",
    "\n",
    "ou de manière équivalente \n",
    "\n",
    "\\begin{align}\n",
    "P(X,Y|Z) = P(X|Z)P(Y|Z)\n",
    "\\end{align}\n",
    "\n",
    "L'hypothèse de Bayes naïve est un exemple d'indépendance conditionnelle . Cette hypothèse est faite dans le contexte d'un algorithme d'apprentissage pour apprendre à classer les emails entre des spams ou des non-spams. Il suppose que la probabilité d'un mot $x$ apparaissant dans un email est conditionnellement indépendante de la présence d'un autre mot $y$ que l'email soit un spam ou non. Ceci n'est évidemment pas juste, car certains mots viennent presque invariablement par paire. Cependant, il s'avère que faire cette hypothèse simplificatrice ne nuit pas beaucoup à la performance et, en tout cas, nous permet d'apprendre à classer les spams rapidement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Règle de chainage et règle de Bayes\n",
    "\n",
    "Nous présentons maintenant deux règles fondamentales mais importantes pour les manipulations de distributions conjointes et de distributions conditionnelles. Le premier est connu comme la règle de chaînage. \n",
    "\n",
    "**Théorème 1**\n",
    "\n",
    "\\begin{align}\n",
    "P(X_1,x_2,\\dots,X_n) = P(X_1)P(X_2|X_1)P(X_3|X_2,X_1) \\dots P(X_n|X_1,\\dots,X_{n-1})\n",
    "\\end{align}\n",
    "\n",
    "La règle des chaînage est souvent utilisée pour évaluer la probabilité conjointe de certaines variables aléatoires, et elle est particulièrement utile lorsqu'il existe une indépendance (conditionnelle) entre les variables. Notez qu'il y a un choix dans l'ordre dans lequel nous démêlons les variables aléatoires lors de l'application de la règle de chaînage; Choisir le bon ordre peut souvent rendre l'évaluation de la probabilité beaucoup plus facile.\n",
    "\n",
    "La deuxième règle que nous allons introduire est la règle de Bayes. La règle de Bayes nous permet de calculer la probabilité conditionnelle $P (X | Y)$ à partir de $P (Y | X)$, dans un sens en \"inversant\" les conditions. \n",
    "\n",
    "**Théorème 2**\n",
    "\n",
    "\\begin{align}\n",
    "P(X|Y)= \\frac{P(Y|X)P(X)}{P(Y)}\n",
    "\\end{align}\n",
    "\n",
    "Démonstration:\n",
    "\n",
    "\\begin{align}\n",
    "& P(X|Y) = \\frac{P(X,Y)}{P(Y)}\\\\ \n",
    "\\text{ or }  & P(Y|X) = \\frac{P(Y,X)}{P(X)} \\\\\n",
    "\\text{ s'en suit } & P(Y,X) = P(Y|X)P(X) = P(X,Y) \\\\\n",
    "\\text{ enfin } & P(X|Y)= \\frac{P(Y|X)P(X)}{P(Y)}\n",
    "\\end{align}\n",
    "\n",
    "Et rappelez-vous que si $P(Y)$ n'est pas donné, nous pouvons toujours appliquer l'équation:\n",
    "\n",
    "\\begin{align}\n",
    "P(Y)= \\sum_{a \\in Val(X)} P(X=a,Y) = \\sum_{a \\in Val(X)} P(Y|X=a)P(X=a)\n",
    "\\end{align}\n",
    "\n",
    "Cette application est parfois appelée loi de probabilité totale.\n",
    "\n",
    "Étendre la règle de Bayes au cas de plusieurs variables aléatoires peut parfois être difficile. Pour être clair, nous allons donner quelques exemples.\n",
    "\n",
    "**Exemple 7**:\n",
    "\n",
    "\\begin{align}\n",
    "P(X,Y|Z)&=\\frac{P(Z|X,Y)P(X,Y)}{P(Z)}    = \\frac{P(Y,Z|X)P(X)}{P(Z)} \\\\\n",
    "P(X|Y,Z)&=\\frac{P(Y|X,Z)P(X,Z)}{P(Y,Z)}  = \\frac{P(Y|X,Z)P(X|Z)P(Z)}{P(Y|Z)P(Z)} = \\frac{P(Y|X,Z)P(X|Z)}{P(Y|Z)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
